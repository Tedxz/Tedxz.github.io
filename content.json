{"meta":{"title":"Zheng Xie's Blog","subtitle":"a river crab writing codes","description":"a river crab writing codes","author":"Zheng Xie","url":"http://tedxz.github.io"},"pages":[{"title":"About Me","date":"2017-01-10T15:31:18.000Z","updated":"2017-01-10T15:31:18.000Z","comments":true,"path":"about/index.html","permalink":"http://tedxz.github.io/about/index.html","excerpt":"","text":"Zheng XIE | 解铮With a passion for machine learning, currently I am studying for a master’s degree at Nanjing University, LAMDA Group, which is led by Prof. Zhi-Hua Zhou. My research interests including machine learning and data mining. I’m interested in coding for long years since I was in senior school. I competed in Olympiad in Informatics, and then in ACM-ICPC with my excellent partners after I entered Xi’an Jiaotong University. I also served as a assistant coach for competitive programming in XJTU. I have worked as a research intern at Baidu Research, Big Data Lab for months in 2015, working on mining spatial-temporal big data for user behavior analysis with machine learning techniques. In 2016, I received my B.Eng degree in Computer Science from XJTU. Some of my achievement could be found at my academic homepage, and some interesting stuff are on my Github homepage. ContactE-mail: xie.zheng777@gmail.comSite: Tedxz.github.ioGithub: github.com/Tedxz"},{"title":"","date":"2016-09-10T08:56:41.852Z","updated":"2015-05-13T05:16:40.109Z","comments":true,"path":"image/2015-05-12-dmlab-image-classification/DMLAB-speech.html","permalink":"http://tedxz.github.io/image/2015-05-12-dmlab-image-classification/DMLAB-speech.html","excerpt":"","text":"图像分类答辩稿 #1 老师您好，本次实验我们组的选题是图像分类问题。下面开始我们的答辩。由于我们的算法训练大约需要5分钟，我们这边先开始运行。 #2 本次的实验问题是对三种类别的图像进行分类，分别是：照片、点线图、剪贴画。 #3 关于图像分类问题，是一个国内外被广泛研究的问题。根据我们查到的相关文献，图像分类问题常用机器学习领域的各种分类算法，如...等。图像分类的一个重要的问题是图像特征的提取，基于内容的图像分类常常在图片不同的色彩空间中提取诸如纹理、色彩数等不同特征以区分不同类型的图片。本次我们的问题比较简单，我们所要分类的图像有比较明显的差异，用到的方法也会简单一些。 #4 在实验的初期首先是方法的选择。常用的分类算法有以下这么几种：...。（停顿） #5 我们来看看这些方法。神经网络对于我们的问题来说过于复杂，计算代价也比较大。KNN算法是一个计算密集的算法，时间开销太大。KMeans是一种无监督的聚类算法，在这里不太合适。决策树还可以，但是决策树分类时一次只用到一个特征，在特征空间划分的时候决策边界是垂直于坐标轴的，不够灵活。我们考虑Logistic回归和支持向量机是两种合适的算法，但是基于进一步的考虑，我们说SVM在创建分类器的时候，只考虑距离分类器最近的点，而Logistic回归考虑到了所有样例。在我们的特征可能不能完全表征图片特性的时候，我们可能做不到对三类图完美分类，那么soft-margin SVM找到的分类器效果可能会大打折扣。编程语言的选择，我们使用Octave，它跟Matlab是相似的。 #6 那么Logistic回归是一个常用的分类算法，它在高维空间寻找一个超平面将我们的不同类别的点划分开来使得总体代价最小，这个代价基于样例到分类器的距离套上sigmoid函数计算。实践当中Logistic回归用到梯度下降算法训练模型，比较好实现，那么有下面几个问题：...。 #7 Logistic回归是一个二元分类器。多元分类问题中常用one vs all的方法，对于每个类别创建分类器。Logistic回归在高维空间创建的分类器是线性的，我们可以通过添加多项式特征的手段使其具有创造高维空间中曲面的能力。对于过度拟合问题，我们可以通过Regularization克服这个问题。 #8 实验第一步是数据集的收集。我们收集的图片共有139张，划分成训练和测试两部分。那么在实验早期的时候我们对于见贴画这个类别仅仅收集一批风格都十分相似的图片，每个类别训练集约有20张，测试集有5张；我们早期的模型在这个数据集上正确率达到了100%，有一些其他的组也使用的是我们早期收集的数据集。后来我们扩展了数据集，收集了风格差异更大的一些图片。 #9 接下来是我们的特征提取。我们的特征提取了以下5个：...。这些是经过我们实验，效果比较好的。我们观察三种不同类别的图片可以看出来，剪贴画是色块之间对比度比较明显，色块内部颜色单一不变化；相比之下照片的色彩变化比较连续，即使是同一种颜色的物体，由于光线和噪声，色块内部的每个像素颜色也有差别。而点线图通常有一个浅色的大面积的纯色作为背景，其他颜色分布较少。基于这些分析，我们也参考了一些文献，提取了色相和饱和度的直方图特征，饱和度的方差，亮度的均值以及亮度在直方图上的离散情况作为图像的特征。其中直方图特征并没有将直方图的每一个数据插入到特征向量，而是设置一个阈值并统计高于该阈值的区间的个数；这样可以很大程度上减少计算开销。 #10 我们早期只使用这5个特征，进行Logistic回归的时候我们做到了这样的准确度（幻灯片右下角）。由于线性不可分等原因这个正确率不是很高，但是可以看出基于这五个特征一定程度上表达的这三类图像之间的区别。 #11 （切出去看octave figure 1 &amp; 2）这是把训练集的图片以其中三个特征为坐标画在了三维空间中。可以看出来照片的色相分布和亮度分布比较大，点线图的亮度均值比较大且集中，饱和度直方图特征比较小。第二个图是取了另外三个特征，因为一张图没办法滑出来5个特征。可以看到虽然所有的样例还是不能线性可分，但是在特征空间中体现出了明显的差别。 #12 那么我们的工作显然还没有完成。我们对模型进一步优化。首先是添加多项式特征，使得我们的分类器一定程度上具有克服线性不可分数据的能力。处于减少计算代价的考虑，我们仅仅加入了二次的特征。接下来，我们将特征的取值做归一化，使得所有特征的取值在[-1, 1]之间。这一优化可以加速梯度下降算法的收敛，优化算法效果。第三步是进行Regularization克服潜在发生的过度拟合。我们通过在交叉验证集上的表现挑选参数lambda，发现lambda等于0的时候，也就是不进行regularization的时候效果最好，说明我们的模型没有严重的过度拟合问题，可能稍有欠拟合的问题。做到这一步，我们对模型测试，得到了一个比较满意的结果。 #13 可以看到，我们的准确率...。如果今后对这个模型进一步优化的话，我们可以尝试添加不同的图片特征，以及更高次的多项式特征。更进一步，我们可以选择能力更强的模型，比如神经网络。总的来说，实验达到了不错的效果。 #14 我们的程序应该跑的差不多了，我们来看一下。（切octave跑一下测试） #15 我们小组今天的答辩就到这里，谢谢老师。"}],"posts":[{"title":"机器学习中的“Margin”","slug":"large-margin-distribution-learning","date":"2016-07-21T13:29:29.000Z","updated":"2016-07-27T16:09:56.890Z","comments":true,"path":"2016/07/21/large-margin-distribution-learning/","link":"","permalink":"http://tedxz.github.io/2016/07/21/large-margin-distribution-learning/","excerpt":"在机器学习领域中，提到“Margin”（间隔）一词，首先想到的常常是SVM（Support Vector Machine，支持向量机）。SVM的基本想法是极大化划分超平面与两类样例之间的最小距离，即所谓的“Margin”，以增强算法的鲁棒性。在此之外，Boosting作为一种集成学习方法，通过将若干弱分类器集成产生一个强分类器，在实践中被证明效果极好。研究发现，AdaBoost算法在训练误差达到0后继续训练能够进一步提升模型的泛化性能。目前对于这一现象，接受程度最广的解释是便是Margin Theory（间隔理论）。","text":"在机器学习领域中，提到“Margin”（间隔）一词，首先想到的常常是SVM（Support Vector Machine，支持向量机）。SVM的基本想法是极大化划分超平面与两类样例之间的最小距离，即所谓的“Margin”，以增强算法的鲁棒性。在此之外，Boosting作为一种集成学习方法，通过将若干弱分类器集成产生一个强分类器，在实践中被证明效果极好。研究发现，AdaBoost算法在训练误差达到0后继续训练能够进一步提升模型的泛化性能。目前对于这一现象，接受程度最广的解释是便是Margin Theory（间隔理论）。 Boosting: Explained By Margin Theory克服过度拟合（overfitting）是机器学习中面临的一个主要问题。SVM与对率回归（Logistic Regression）中的正则化（regularization）、神经网络训练中的早停（early stopping）与Drop Out，决策树中的各种剪枝手段都是为了克服过度拟合而发明的方法。一般情况下，当训练误差达到0之后，继续训练会有另模型过渡拟合训练数据的风险，因此需要及时停止。而AdaBoost在训练误差到0之后继续训练可以进一步提升泛化性能。 令X\\mathcal{X}X和Y\\mathcal{Y}Y代表样例的输入和输出空间，DDD是X×Y\\mathcal{X} \\times \\mathcal{Y}X×Y上的样例真实分布，而S={(x1,y1),…,(xm,ym)}S={\\lbrace(\\mathbf{x}_1,y_1), \\ldots, (\\mathbf{x}_m,y_m)\\rbrace}S={(x​1​​,y​1​​),…,(x​m​​,y​m​​)}则是DDD上的抽样。在假设空间H\\mathcal{H}H中，基分类器h:X→Yh:\\mathcal{X} \\rightarrow \\mathcal{Y}h:X→Y进行加权组合形成的集成分类器f∈C(H)f \\in \\mathcal{C}(\\mathcal{H})f∈C(H)，即H\\mathcal{H}H的凸包。在AdaBoost中，集成分类器f(x)f(x)f(x)由一系列基分类器加权投票产生结果，即： f(x)=∑iαihi(x).f(\\mathbf{x})=\\sum_i \\alpha_i h_i(\\mathbf{x}).f(x)=​i​∑​​α​i​​h​i​​(x). 其中， ∑iαi=1,αi≥0.\\sum_i \\alpha_i=1, \\alpha_i \\geq 0.​i​∑​​α​i​​=1,α​i​​≥0. 为了解释这一现象，1998年，Schapire等首次提出间隔理论（Margin Theory for Boosting）[2]。基于上述的强分类器定义，可以如下定义间隔： yf(x)=∑i:y=hi(x)αi−∑i:y≠hi(x)αi,yf(\\mathbf{x})=\\sum_{i:y=h_i(\\mathbf{x})} \\alpha_i - \\sum_{i:y \\neq h_i(\\mathbf{x})} \\alpha_i,yf(x)=​i:y=h​i​​(x)​∑​​α​i​​−​i:y≠h​i​​(x)​∑​​α​i​​, 即，正确投票与错误投票的加权差。当yf(x)&lt;0yf(\\mathbf{x})&lt;0yf(x)&lt;0，即为对x\\mathbf{x}x错误分类。直觉上讲，随着训练，当在所有样例上大多数基分类器能够做出正确的判断时，算法即可达到0训练误差，但是此时基分类器可能还处于欠拟合的状态，继续训练可以进一步发挥基分类器的潜力。当上述定义的间隔越大，分类器对其做出的判断把握就越大。Schapire等基于理论推导，给出了一个AdaBoost泛化误差的约束（bound）。 定理 1 对于任意δ&gt;0\\delta&gt;0δ&gt;0和θ&gt;0\\theta&gt;0θ&gt;0，以至少1−δ1-\\delta1−δ的概率在大小为mmm的抽样SSS上，每个集成学习器f∈C(H)f \\in \\mathcal{C}(\\mathcal{H})f∈C(H)满足如下约束：PrD[yf(x)&lt;0]≤PrS[yf(x)&lt;θ]+O(1m(lnmln∣H∣θ2+ln1δ)12)Pr_D[yf(\\mathbf{x})&lt;0] \\leq Pr_S[yf(\\mathbf{x})&lt;\\theta]+O\\left( \\frac{1}{\\sqrt{m}} \\left( \\frac{\\ln m \\ln |\\mathcal{H}|}{\\theta^2}+\\ln{\\frac{1}{\\delta}} \\right)^{\\frac{1}{2}} \\right)Pr​D​​[yf(x)&lt;0]≤Pr​S​​[yf(x)&lt;θ]+O(​√​m​​​​​1​​(​θ​2​​​​lnmln∣H∣​​+ln​δ​​1​​)​​2​​1​​​​) 这一理论在当时受到了广泛关注。根据这一公式，如果能确保所有样例上最小的间隔尽可能大，则泛化误差可以被确定到较低的范围。由此，在1999年Breiman发表的论文中，着重考虑最小间隔，进一步收紧了泛化误差的约束[3]。 定理 2 对于任意δ&gt;0\\delta&gt;0δ&gt;0，若θ=y^1f(x^1)&gt;42∣H∣\\theta=\\hat{y}_1 f(\\hat{\\mathbf{x}}_1) &gt; 4\\sqrt{\\frac{2}{|\\mathcal{H}|}}θ=​y​^​​​1​​f(​x​^​​​1​​)&gt;4√​​∣H∣​​2​​​​​且R≤2mR\\leq 2mR≤2m，以至少1−δ1-\\delta1−δ的概率在大小为mmm的抽样SSS上，每个集成学习器f∈C(H)f \\in \\mathcal{C}(\\mathcal{H})f∈C(H)满足如下约束：PrD[yf(x)&lt;0]≤R(ln(2m)+ln1R+1)+1mln∣H∣δPr_D[yf(\\mathbf{x})&lt;0] \\leq R \\left(\\ln(2m)+\\ln \\frac{1}{R} +1\\right) + \\frac{1}{m} \\ln \\frac{|\\mathcal{H}|}{\\delta}Pr​D​​[yf(x)&lt;0]≤R(ln(2m)+ln​R​​1​​+1)+​m​​1​​ln​δ​​∣H∣​​ 其中R=32ln2∣H∣mθ2R=\\frac{32\\ln 2|\\mathcal{H}|}{m \\theta^2}R=​mθ​2​​​​32ln2∣H∣​​。 这似乎更加强调了最小间隔的重要性。为了验证最小间隔对泛化误差的影响，Breiman设计了arc-gv算法——该算法对AdaBoost进行了修改，直接最大化最小间隔——并进行实验。然而，实验结果却是AdaBoost在几乎所有情况下泛化误差都远远优于arc-gv。于是，Breiman表达了对间隔理论这一解释的质疑，间隔理论的研究趋于沉寂。 2006年，Reyzin与Schapire发现Breiman的实验中，两种算法所使用的基分类器——CART决策树——的深度有所差异，即两种集成学习算法的基分类器有着不同的模型复杂度[4]。他们重新设计了实验，将基分类器限定为决策树桩（decision stumps）重新进行了验证。实验结果表明，虽然arc-gv得到的分类器有着更大的最小间隔，但是对于所有样例的间隔分布（margin distribution），即间隔的均值和方差，却比AdaBoost得到分类器的间隔分布差得多。为了说明间隔分布的重要性，Wang等在2008年提出了Emargin[5]，即arginfq∈{q0,q0+1m,⋯,1}KL−1(q;u[θ^(q)])arginf_{q \\in \\{q_0, q_0 + \\frac{1}{m}, \\cdots, 1\\}} KL^{-1}(q;u[\\hat{\\theta}(q)])arginf​q∈{q​0​​,q​0​​+​m​​1​​,⋯,1}​​KL​−1​​(q;u[​θ​^​​(q)])，并基于此概念进一步收紧了泛化误差的约束。 定理 3 对于任意δ&gt;0\\delta&gt;0δ&gt;0，若8&lt;∣H∣&lt;∞8&lt;|\\mathcal{H}|&lt;\\infty8&lt;∣H∣&lt;∞，以至少1−δ1-\\delta1−δ的概率在大小为m&gt;1m&gt;1m&gt;1的抽样SSS上，每个集成学习器f∈C(H)f \\in \\mathcal{C}(\\mathcal{H})f∈C(H)满足如下约束：PrD[yf(x)&lt;0]≤∣H∣m+infq∈{q0,q0+1m,⋯,1}KL−1(q;u[θ^(q)])Pr_D[yf(\\mathbf{x})&lt;0] \\leq \\frac{|\\mathcal{H}|}{m} + \\inf_{q \\in \\{q_0, q_0 + \\frac{1}{m}, \\cdots, 1\\}} KL^{-1}(q;u[\\hat{\\theta}(q)])Pr​D​​[yf(x)&lt;0]≤​m​​∣H∣​​+​q∈{q​0​​,q​0​​+​m​​1​​,⋯,1}​inf​​KL​−1​​(q;u[​θ​^​​(q)]) 其中q0=PrS[yf(x)≤8∣H∣]&lt;1q_0 = Pr_S [yf(\\mathbf{x}) \\leq \\sqrt{\\frac{8}{|\\mathcal{H}|}}]&lt;1q​0​​=Pr​S​​[yf(x)≤√​​∣H∣​​8​​​​​]&lt;1，u[θ^(q)]=1m(8ln∣H∣θ^2(q)ln2m2ln∣H∣+ln∣H∣+lnmδ)u[\\hat{\\theta}(q)]=\\frac{1}{m}\\left( \\frac{8\\ln |\\mathcal{H}|}{\\hat{\\theta}^2(q)}\\ln \\frac{2m^2}{\\ln |\\mathcal{H}|} + \\ln |\\mathcal{H}| + \\ln \\frac{m}{\\delta} \\right)u[​θ​^​​(q)]=​m​​1​​(​​θ​^​​​2​​(q)​​8ln∣H∣​​ln​ln∣H∣​​2m​2​​​​+ln∣H∣+ln​δ​​m​​)，θ^(q)=sup{θ∈(8∣H∣,1]:PrS[yf(x)≤θ]≤q}\\hat{\\theta}(q)=\\sup\\{\\theta \\in \\left(\\sqrt{\\frac{8}{|\\mathcal{H}|}},1\\right]:Pr_S[yf(\\mathbf{x})\\leq \\theta]\\leq q\\}​θ​^​​(q)=sup{θ∈(√​​∣H∣​​8​​​​​,1]:Pr​S​​[yf(x)≤θ]≤q}。 对于该定理的详细内容可以参阅参考文献。由于Emargin的概念比较抽象，根据该理论对算法进行优化有一定的难度。 在此之后，高尉老师与周志华老师在2013年将理论进一步推进[6]。他们发现无论是最小间隔，还是Emargin，都是第k间隔的一个特例；通过考虑间隔的分布，即均值与方差，可以进一步收紧约束。 定理 4 对于任意δ&gt;0\\delta&gt;0δ&gt;0，若8&lt;∣H∣&lt;∞8&lt;|\\mathcal{H}|&lt;\\infty8&lt;∣H∣&lt;∞，以至少1−δ1-\\delta1−δ的概率在大小为m&gt;5m&gt;5m&gt;5的抽样SSS上，每个集成学习器f∈C(H)f \\in \\mathcal{C}(\\mathcal{H})f∈C(H)满足如下约束：PrD[yf(x)&lt;0]≤2m+infθ∈(0,1][PrS[yf(x)&lt;θ]+7μ+33μ3m+3μmPrS[yf(x)&lt;θ]]Pr_D[yf(\\mathbf{x})&lt;0] \\leq \\frac{2}{m} + \\inf_{\\theta \\in (0,1]} \\left[Pr_S[yf(\\mathbf{x})&lt;\\theta] + \\frac{7\\mu +3\\sqrt{3\\mu}}{3m}+\\sqrt{\\frac{3\\mu}{m}Pr_S[yf(\\mathbf{x})&lt;\\theta]} \\right]Pr​D​​[yf(x)&lt;0]≤​m​​2​​+​θ∈(0,1]​inf​​[Pr​S​​[yf(x)&lt;θ]+​3m​​7μ+3√​3μ​​​​​+√​​m​​3μ​​Pr​S​​[yf(x)&lt;θ]​​​] 其中μ=8θ2lnmln(2∣H∣)+ln2∣H∣δ\\mu=\\frac{8}{\\theta^2}\\ln m \\ln (2|\\mathcal{H}|)+\\ln \\frac{2|\\mathcal{H}|}{\\delta}μ=​θ​2​​​​8​​lnmln(2∣H∣)+ln​δ​​2∣H∣​​。 其工作基于间隔分布推出了比Schapire与Breiman更紧的约束，即解释了arc-gv泛化误差劣于AdaBoost的原因。根据其论文，较大的间隔均值和较小的间隔方差意味着更好的间隔分布。这一理论工作对Breiman对间隔理论的质疑给出了明确的答案，也很好地解释了AdaBoost在训练误差达到0之后继续训练能够提升泛化性能的原因，也指明了当间隔分布无法继续提升时，过度拟合终将发生。 SVM: A Large Margin MethodSVM[7]凭借其良好的分类效果，一直受到了广泛的关注。其基本想法是极大化划分超平面与两类样例之间的最小距离，即所谓的“Margin”。 对于线性可分的情况，硬间隔（hard-margin）SVM最大化最小间隔，通过缩放w\\mathbf{w}w令间隔y(w⊺x+b)=1y(\\mathbf{w}^\\intercal \\mathbf{x}+b)=1y(w​⊺​​x+b)=1。最小间隔真实大小1∣w∣\\frac{1}{|\\mathbf{w}|}​∣w∣​​1​​的最大化则可以转化为下述优化目标：minw12w⊺w\\min_{\\mathbf{w}} \\frac{1}{2}\\mathbf{w}^\\intercal \\mathbf{w}​w​min​​​2​​1​​w​⊺​​w s.t. yiw⊺ϕ(xi)≥1,\\text{s.t. } y_i \\mathbf{w}^\\intercal \\phi(\\mathbf{x}_i) \\geq 1 ,s.t. y​i​​w​⊺​​ϕ(x​i​​)≥1, i=1,⋯,m.i=1,\\cdots,m.i=1,⋯,m. 对于线性不可分的情况，引入松弛变量允许样例被错误的分类，并计算代价：minw,ξi12w⊺w+C∑i=1mξi\\min_{\\mathbf{w}, \\xi_i} \\frac{1}{2}\\mathbf{w}^\\intercal \\mathbf{w} + C\\sum_{i=1}^m \\xi_i​w,ξ​i​​​min​​​2​​1​​w​⊺​​w+C​i=1​∑​m​​ξ​i​​ s.t. yiw⊺ϕ(xi)≥1−ξi,\\text{s.t. } y_i \\mathbf{w}^\\intercal \\phi(\\mathbf{x}_i) \\geq 1 - \\xi_i ,s.t. y​i​​w​⊺​​ϕ(x​i​​)≥1−ξ​i​​, ξi≥0, i=1,⋯,m.\\xi_i \\geq 0,\\text{ }i=1,\\cdots,m.ξ​i​​≥0, i=1,⋯,m. 此处不再赘述SVM的相关内容。值得一提的是，根据上述公式，SVM的间隔也可以表示成yf(x)yf(\\mathbf{x})yf(x)的形式：yf(x)=y(w⊺x+b)yf(\\mathbf{x})=y(\\mathbf{w}^\\intercal \\mathbf{x}+b)yf(x)=y(w​⊺​​x+b)此时，间隔的意义是样例点到划分超平面的距离。 ODM: Combining The Ideas两种“Margin”有着相同的名字和形式（yf(x)yf(\\mathbf{x})yf(x)），然而，其意义大不相同。Boosting算法中的“Margin”含义为多个基分类器正确投票的“绝对优势”，而SVM中的“Margin”是样例点到划分超平面的距离。虽然两种“Margin”都给人一种“正确划分样例的把握大小”的感觉，但其理论推导并不能照搬。 虽然如此，两个“Margin”形式的相似性仍然可以带来一些算法改进的灵感。张腾师兄最近的工作ODM（Optimal margin Distribution Machine）[9]及其早期工作LDM（Large margin Distribution Machine，在后续工作中改称ODML）[8]将SVM中间隔（样例点到划分超平面的距离）的分布纳入考虑，以期望提升算法的泛化性能。 LDM（ODML）在SVM的目标函数上增加了均值（mean）项和方差（variance）项，以期望获得间隔均值更大且方差更小的分类器。另X=[ϕ(x1),⋯,ϕ(xm)]\\mathbf{X}=[\\phi(\\mathbf{x}_1),\\cdots,\\phi(\\mathbf{x}_m)]X=[ϕ(x​1​​),⋯,ϕ(x​m​​)]，则均值项可以表示为：γ¯=1m∑i=1myiw⊺ϕ(xi)=1m(Xy)⊺w\\bar{\\gamma} = \\frac{1}{m}\\sum_{i=1}^{m} y_i \\mathbf{w}^\\intercal \\phi(\\mathbf{x}_i) = \\frac{1}{m}(\\mathbf{Xy})^\\intercal \\mathbf{w}​γ​¯​​=​m​​1​​​i=1​∑​m​​y​i​​w​⊺​​ϕ(x​i​​)=​m​​1​​(Xy)​⊺​​w 方差项可表示为：γ^=1m2∑i=1m∑j=1m(yiw⊺ϕ(xi)−yjw⊺ϕ(xj))2=2m2(mw⊺XX⊺w−w⊺Xyy⊺X⊺w)\\hat{\\gamma} = \\frac{1}{m^2}\\sum_{i=1}^{m} \\sum_{j=1}^{m} (y_i \\mathbf{w}^\\intercal \\phi(\\mathbf{x}_i) - y_j \\mathbf{w}^\\intercal \\phi(\\mathbf{x}_j))^2 = \\frac{2}{m^2}(m\\mathbf{w^\\intercal XX^\\intercal w} - \\mathbf{w ^\\intercal Xyy^\\intercal X^\\intercal w})​γ​^​​=​m​2​​​​1​​​i=1​∑​m​​​j=1​∑​m​​(y​i​​w​⊺​​ϕ(x​i​​)−y​j​​w​⊺​​ϕ(x​j​​))​2​​=​m​2​​​​2​​(mw​⊺​​XX​⊺​​w−w​⊺​​Xyy​⊺​​X​⊺​​w) 则软间隔LDM的优化目标为：minw,ξi12w⊺w+C∑i=1mξi−λ1γ¯+λ2γ^\\min_{\\mathbf{w}, \\xi_i} \\frac{1}{2}\\mathbf{w}^\\intercal \\mathbf{w} + C\\sum_{i=1}^m \\xi_i -\\lambda_1 \\bar{\\gamma} + \\lambda_2 \\hat{\\gamma}​w,ξ​i​​​min​​​2​​1​​w​⊺​​w+C​i=1​∑​m​​ξ​i​​−λ​1​​​γ​¯​​+λ​2​​​γ​^​​ s.t. yiw⊺ϕ(xi)≥1−ξi,\\text{s.t. } y_i \\mathbf{w}^\\intercal \\phi(\\mathbf{x}_i) \\geq 1 - \\xi_i ,s.t. y​i​​w​⊺​​ϕ(x​i​​)≥1−ξ​i​​, ξi≥0, i=1,⋯,m.\\xi_i \\geq 0,\\text{ }i=1,\\cdots,m.ξ​i​​≥0, i=1,⋯,m.十分显然，当参数λ1\\lambda_1λ​1​​和λ2\\lambda_2λ​2​​取0时，LDM即退化成为SVM。 ODM简化了LDM的最优化目标的形式。ODM不再将最小间隔、间隔均值和方差同时考虑；而是通过缩放w\\mathbf{w}w令间隔均值为1，最小化w\\mathbf{w}w以最大化间隔均值；同时对远离均值的样例按照距离计算损失。其优化目标如下：minw,ξi,ϵi12w⊺w+1m∑i=1m(C1ξi2+C2ϵi2)\\min_{\\mathbf{w}, \\xi_i, \\epsilon_i} \\frac{1}{2}\\mathbf{w}^\\intercal \\mathbf{w} + \\frac{1}{m} \\sum_{i=1}^m (C_1 \\xi_i^2 + C_2 \\epsilon_i^2)​w,ξ​i​​,ϵ​i​​​min​​​2​​1​​w​⊺​​w+​m​​1​​​i=1​∑​m​​(C​1​​ξ​i​2​​+C​2​​ϵ​i​2​​) s.t. yiw⊺ϕ(xi)≥1−D−ξi,\\text{s.t. } y_i \\mathbf{w}^\\intercal \\phi(\\mathbf{x}_i) \\geq 1 - D - \\xi_i ,s.t. y​i​​w​⊺​​ϕ(x​i​​)≥1−D−ξ​i​​, s.t. yiw⊺ϕ(xi)≤1+D+ϵi,\\phantom{\\text{s.t. }} y_i \\mathbf{w}^\\intercal \\phi(\\mathbf{x}_i) \\leq 1 + D + \\epsilon_i ,s.t. y​i​​w​⊺​​ϕ(x​i​​)≤1+D+ϵ​i​​, i=1,⋯,m.\\text{ }i=1,\\cdots,m. i=1,⋯,m.当样例与间隔均值的位置距离超过DDD，则累计损失。DDD的引入将LDM相较于SVM牺牲掉的稀疏性挽回了一部分。由于样例向远离划分平面的方向偏离不倾向于导致误分类，因此通过C1C_1C​1​​和C2C_2C​2​​来区分两侧不同的代价。 ODM与LDM启发于Boosting的间隔理论；但其泛化误差约束推导并不相同。感兴趣的读者可以在参考文献中找到推导。 Reference[1] Zhou Z.-H. “Large margin distribution learning.” IAPR Workshop on Artificial Neural Networks in Pattern Recognition. Springer International Publishing, 2014. [2] Schapire R. E., Freund Y., Bartlett P., Lee W. S. “Boosting the margin: A new explanation for the effectiveness of voting methods.” Annals of statistics (1998): 1651-1686. [3] Breiman L. “Prediction games and arcing algorithms.” Neural computation 11.7 (1999): 1493-1517. [4] Reyzin L., Schapire R. E. “How boosting the margin can also boost classifier complexity.” Proceedings of the 23rd international conference on Machine learning. ACM, 2006. [5] Wang L., Sugiyama M., Yang C., Zhou Z. H., Feng J. “On the Margin Explanation of Boosting Algorithms.” Proceedings of 21st International Conference on Learning Theory. 2008. [6] Gao W., Zhou Z.-H. “On the doubt about margin explanation of boosting.” Artificial Intelligence 203 (2013): 1-18. [7] Corinna C., Vapnik V. “Support-vector networks.” Machine learning 20.3 (1995): 273-297. [8] Zhang T., Zhou Z.-H. “Large margin distribution machine.” Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014. [9] Zhang T., Zhou Z.-H. “Optimal Margin Distribution Machine.” arXiv preprint arXiv:1604.03348 (2016).","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://tedxz.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://tedxz.github.io/tags/Machine-Learning/"}]},{"title":"《秘密》- 东野圭吾","slug":"secret-higashino-keiko","date":"2016-05-05T13:16:29.000Z","updated":"2016-05-05T17:58:01.753Z","comments":true,"path":"2016/05/05/secret-higashino-keiko/","link":"","permalink":"http://tedxz.github.io/2016/05/05/secret-higashino-keiko/","excerpt":"我对文学的感觉从小就很不突出，很长一段时间也不怎么接触文学作品。上个假期心血来潮搜寻了几本推理小说放在Kindle里，结果一发不可收拾。虽然接触的作品很少，但是其中东野的作品每一部都爱不释手。开学忙碌起来Kindle又很少拿出来看，但昨天在火车上偶然打开的一本，又让我一口气读完到深夜。前面读过了《嫌疑人X的献身》《解忧杂货铺》《白夜行》《超·杀人事件》，这本《秘密》算是我读的第五本东野的小说。","text":"我对文学的感觉从小就很不突出，很长一段时间也不怎么接触文学作品。上个假期心血来潮搜寻了几本推理小说放在Kindle里，结果一发不可收拾。虽然接触的作品很少，但是其中东野的作品每一部都爱不释手。开学忙碌起来Kindle又很少拿出来看，但昨天在火车上偶然打开的一本，又让我一口气读完到深夜。前面读过了《嫌疑人X的献身》《解忧杂货铺》《白夜行》《超·杀人事件》，这本《秘密》算是我读的第五本东野的小说。 开篇讲述男主人公杉田平介的妻女遭遇车祸，令人惊奇的是妻子直子的灵魂进入了女儿藻奈美的身体。虽然直子让丈夫明白了在女儿身体里的是自己的灵魂，但是这种灵异的事件却不能被世人接受。于是直子决定以藻奈美的身份继续生活，还决心成为一个自强自立的女人。平介爱着自己的妻子，虽然有重新开始爱情的机会，但是他最终的选择还是陪伴直子。妻子在女儿的身体里每日努力成为一个优秀的学生，却越来越让丈夫感觉两人之间存在无法跨越的鸿沟。女儿身体里的妻子有自己的生活，有她的追求者，虽然她并没有背叛丈夫的举动，但丈夫却难以压制心中关于妻子将要疏远自己的不安和疑虑。因为女儿的身体，两人连性爱也无法进行。两人的关系几乎已经无可调和。 一个偶然的机会，让丈夫的心情有了转变。车祸肇事司机的前妻根岸典子联系到平介，将她生下与前男友的儿子，知道真相无法接受而离开的丈夫却为供儿子上大学拼命工作的来龙去脉告诉了他。 “当我知道我和文也没有血缘关系时，我只是一味地考虑有没有心情当他的父亲，却没有想过让心爱的人幸福。我这么喜欢文也，却选择离开了他，我真傻……” 根岸典子回忆着引发车祸而死去的前夫的话。自己是不是只顾考虑自己的心情，却忘记了让心爱的人幸福？回到家里，他让妻子去参加学校的旅行，也把称呼从“直子”改成了“藻奈美”——他明白了让直子好好地以藻奈美的身份生活，才是让她幸福的做法。 而从那以后，女儿藻奈美的意识却开始出现，并越来越长的占据她原本的身体。读到这里，我想，也许直子的意识完全离开，平介和藻奈美也最终能过上正常而幸福的生活吧。一段时间以后，直子感觉自己的意识已经再也不会回来，约丈夫一起在初次约会的公园一起度过了直子的意识的最后时刻。 从那以后，直子完全离开了平介的生活，藻奈美继续母亲为她打下的基础，成为了优秀的女性，也要和肇事司机的儿子根岸文也结婚了。平介拿着司机的遗物怀表想去店里修好戴着出席婚礼，却听老板说出了不以为然的事情。 “嗯，藻奈美指定要用直子的戒指，重新打造一枚婚戒，她觉得那枚戒指是母亲的遗物。” 藏在藻奈美玩具熊里的戒是自己和直子的秘密啊！藻奈美是不可能知道的！平介心里隐隐意识到了事情的真相。当他回到新娘休息室，和藻奈美四目相对的时候，他一切都明白了。 “爸爸！”她说道，“谢谢您这么长时间、这么长时间以来抚养我长大！”她的声音变得哽咽。 平介平静地回应，淡淡地把未来的女婿叫走。新娘的父亲一定会对新郎揍一拳，但这次，他要揍两拳。 听说男生和女生读到这部小说的时候感觉是不一样的。读到文章中间的时候，平介面对日益远离自己的妻子几近崩溃的感觉令我感同身受。妻子或许对过去的生活不满而发奋成为一个杰出的女性。她专于学业，忙于社团活动的行为无可挑剔；但正因这无可挑剔，丈夫的悲愤无法发泄。几近崩溃的他越想把妻子留住，越是丑态百出——他无法克制地窃听电话、干涉她与追求者见面，却只是一步一步逼爱的人离开。深夜读到这里，我的心中也和平介有着一样的悲愤，只得掏出手机联络高中时代的好友，宣泄一下这愤恨。 当平介终于下定决心，不再坚守自私的占有欲的时候，一切变得明朗起来。直子明白丈夫的良苦用心，决定演一出戏，让陪伴自己多年的丈夫开始新的生活。当两人最后在初次约会的公园相约，直子抛弃了自己的身份——从今以后，直子就从这个世界上死去了。只有这样，才能让丈夫丢下自己这个包袱，开始新的生活。 故事的最后，平介才明白妻子这些年来的付出。只要她不承认是直子，直子就是一个已经死去的灵魂。两人四目相对那一刻，虽然只是父女般的对话，两人心中却明白了一切。文末平介在要打文也两拳之时泪流满面，读者的泪水也夺眶而出。 也许爱情中迷茫之时更要考虑让所爱之人幸福，如果没有其他选择，也要学会放手吧。","categories":[{"name":"Literature","slug":"Literature","permalink":"http://tedxz.github.io/categories/Literature/"}],"tags":[{"name":"Literature","slug":"Literature","permalink":"http://tedxz.github.io/tags/Literature/"},{"name":"Hagashino Keiko","slug":"Hagashino-Keiko","permalink":"http://tedxz.github.io/tags/Hagashino-Keiko/"}]},{"title":"Routing Among Multiple Networks","slug":"routing-between-multiple-networks","date":"2016-04-16T09:41:30.000Z","updated":"2017-01-06T16:09:55.084Z","comments":true,"path":"2016/04/16/routing-between-multiple-networks/","link":"","permalink":"http://tedxz.github.io/2016/04/16/routing-between-multiple-networks/","excerpt":"BackgroundRecently, I’m working on experiments for days in lab. Since I’m not a formal student of NJU yet, I do not have a student account for connecting to campus network, which can access to both internet and group’s intranet. I can access to internet by establishing a PPPoE connection to ChinaUnicom’s network, which kicks me out of group’s intranet, and prevents me from using clusters. Another choice is to stay in the intranet, which I can’t stand as well.","text":"BackgroundRecently, I’m working on experiments for days in lab. Since I’m not a formal student of NJU yet, I do not have a student account for connecting to campus network, which can access to both internet and group’s intranet. I can access to internet by establishing a PPPoE connection to ChinaUnicom’s network, which kicks me out of group’s intranet, and prevents me from using clusters. Another choice is to stay in the intranet, which I can’t stand as well. It’s a common case that you want to connect to both an internet and an intranet, maybe one of them through Wi-Fi and another through a cable. But you fail to access them at the same time. What should you do? SolutionThe solution is to modify your routing table, to use a particular network interface for some address and the other one for the rest. Use route print command in CMD window, you can see the route table on your PC. Here is a part of the output: 12345678910111213141516171819202122IPv4 Route Table===========================================================================Active Routes:Network Destination Netmask Gateway Interface Metric 0.0.0.0 0.0.0.0 172.26.0.1 172.26.69.242 4245 0.0.0.0 0.0.0.0 On-link 153.3.14.18 21 127.0.0.0 255.0.0.0 On-link 127.0.0.1 4531 127.0.0.1 255.255.255.255 On-link 127.0.0.1 4531 127.255.255.255 255.255.255.255 On-link 127.0.0.1 4531 153.3.14.18 255.255.255.255 On-link 153.3.14.18 276 172.26.0.0 255.255.128.0 On-link 172.26.69.242 4501 172.26.69.242 255.255.255.255 On-link 172.26.69.242 4501 172.26.127.255 255.255.255.255 On-link 172.26.69.242 4501 224.0.0.0 240.0.0.0 On-link 127.0.0.1 4531 224.0.0.0 240.0.0.0 On-link 172.26.69.242 4501 224.0.0.0 240.0.0.0 On-link 153.3.14.18 21 255.255.255.255 255.255.255.255 On-link 127.0.0.1 4531 255.255.255.255 255.255.255.255 On-link 172.26.69.242 4501 255.255.255.255 255.255.255.255 On-link 153.3.14.18 276===========================================================================Persistent Routes: None My IP address of the intranet is 172.26.69.242, and the IP address of the internet is 153.3.14.18 in this case. This may refer to the output of command ipconfig /all. In the situation I explained above, I want to connect to clusters whose IP are as 114.*.*.* through the interface 172.26.69.242. 1route add 114.0.0.0 mask 255.0.0.0 172.26.69.242 The above command adds a route, sending all packets whose destination are 114.*.*.* through interface 172.26.69.242. IP RoutingSimply, the computer sending a datagram to a particular destination may have to go through many nodes in the network. To find a possible (and fast) path from your PC to your destination, each node on the path should know what the next step is. A routing table tells the computer where should it send the datagrams next step for a particular destination. IP searches the routing table for the route that is the closest match to the destination IP address. If Network Destination in the table &amp; Netmask = Destination IP address &amp; Netmask, it means that the destination IP address matches the Network Destination. A metric indicates the cost of using a route, which is typically the number of hops to the IP destination. If there are multiple routes to the same destination with different metrics, the route with the lowest metric is selected. In the above case, we added a route with Destination = 144.0.0.0 and Netmask = 255.0.0.0, which would be the closest route for any IP addresses like 114.*.*.*, telling the PC send the datagrams through 172.26.69.242. Related LinksIP routingThe IP routing tableWhat does “On-link” mean on the result of “route print” command?出差常备：route命令详解与使用实例","categories":[{"name":"Skill","slug":"Skill","permalink":"http://tedxz.github.io/categories/Skill/"}],"tags":[{"name":"Skill","slug":"Skill","permalink":"http://tedxz.github.io/tags/Skill/"},{"name":"Network","slug":"Network","permalink":"http://tedxz.github.io/tags/Network/"},{"name":"Windows","slug":"Windows","permalink":"http://tedxz.github.io/tags/Windows/"}]},{"title":"离开北京","slug":"away-from-beijing","date":"2015-10-12T07:51:54.000Z","updated":"2015-12-06T08:22:02.486Z","comments":true,"path":"2015/10/12/away-from-beijing/","link":"","permalink":"http://tedxz.github.io/2015/10/12/away-from-beijing/","excerpt":"吃完晚饭早早地就离开了公司，十月份才七点钟天就完全黑了下来。走出大厦看着夜景突然有一种淡淡的忧伤。下次再回到这个城市不知道是什么时候，或许就是来办理离职了。百度大厦外的夜景可能是最后一次看见，而西二旗到霍营的城铁可能再也没有机会搭乘了。","text":"吃完晚饭早早地就离开了公司，十月份才七点钟天就完全黑了下来。走出大厦看着夜景突然有一种淡淡的忧伤。下次再回到这个城市不知道是什么时候，或许就是来办理离职了。百度大厦外的夜景可能是最后一次看见，而西二旗到霍营的城铁可能再也没有机会搭乘了。 出了地铁站就转进链家地产的商铺，早上约好的“管家”帮我算清我和我的出租屋下一个租客之间的债务。由于手续办理时间早于屋子的交接时间，我需要把下一个租客多交的房租补给他。“管家”说一些零碎的费用他都没有再要我补上，令我对这个只见过一面的大哥有些感激——他也是一个在软件园工作的IT哥。 还记得三个月前来到北京，杰哥陪我找租房。那时候链家的新logo正在更换，绿色牌子还放在门口地上，干净的很好看。链家自如的房子需要“抢”——在开放出租后第一个付款的客户才能租到，考虑到自如的房子装修都一个样，我没看房就果断决定要这间屋子。工作人员果然顺利的帮我“抢”到这间屋子，我便开始了我的北漂生涯。 租到这间屋子之前我们也看了其他的房子，前面看房的位置去上班竟然要爬到城铁铁轨上走很远，这也是让我对现在住的地方很满意的原因之一——出了小区就是地铁站。看房的时候还偶然认识了也在租房的百度的一个师姐；交流起来让人感觉很亲切，还有帮我留意合适的出租房，我喜欢叫她邹姐。后来听说她在立水桥租到了房子。在百度偶遇过几次，仓促间也没能好好聊聊，希望下次会有机会。 离开链家，回到家里。想到明天就要离开这间屋子，心中恋恋不舍，便掏出手机把屋里各处拍了个遍。早就希望能够自己在自己的房子里或单独或和爱人居住，让自己充分自由。这次独自在外居住，果然不辜负之前的幻想。早晨窗户照进的明媚阳光让我明白我也是一个能在早上7点钟自然醒的好少年；晚上下班回来或上网或给自己做顿夜宵也乐在其中。实习前总想着周末要好好加班多挣零花钱；真正到了实习的周末却不愿意再去公司加班了。或宅在家里看剧，饿了就到楼下吃碗牛肉面——也是附近为数不多的吃食之一了；或跑到人山人海的景点逛逛，有时自己一人有时约上好久不见却碰巧在北京忙碌的朋友。合租的三户有一对设计师couple，一对程序员couple（女生不了解，应该不是~），一个建筑师和一只叫油条的喵。平常没有过多交集，偶尔客厅厨房遇到又让人感到不那么硬生生的孤单。我喜欢这个地方，孤独而自由；但在我刚习惯把这个地方叫做“家”的时候，我就要离开了。 来北京是为了实习，因为没有要离职所以也没有太多关于百度的矫情要抒发。在百度的工作不敢妄称在做科研，但是工作的内容从不让我感到无聊。还有好多优秀的同事——今天先不说他们的优秀；而最让我感到暖心的是身边的每一个人都会很耐心的解答我每一个傻到家的问题（笑）。大厦不像宿舍充斥着干扰，让人能静心工作思考；但是因为穷（汗），我不得不回归宿舍远程办公了（sigh）。 现在是晚上1点15分；写到这里感觉心情平静了许多，准备洗洗睡了。在实习期间的工作生活过段日子再好好记录一下。","categories":[{"name":"Diary","slug":"Diary","permalink":"http://tedxz.github.io/categories/Diary/"}],"tags":[{"name":"Diary","slug":"Diary","permalink":"http://tedxz.github.io/tags/Diary/"}]},{"title":"记即将过去的大三下学期","slug":"reivew-of-second-semester-in-junior","date":"2015-06-13T15:38:26.000Z","updated":"2016-07-27T10:04:29.605Z","comments":true,"path":"2015/06/13/reivew-of-second-semester-in-junior/","link":"","permalink":"http://tedxz.github.io/2015/06/13/reivew-of-second-semester-in-junior/","excerpt":"复习累了，码码字放松一下。大三下学期是个充满机遇和选择的学期，不断地纠结，争取，也获得了许多机会。学期快要结束，一个个努力的结果也渐渐到来。 寻找实习 ★★★★★ 保研学校 ★★★★★ 算法竞赛 ★★☆ 课余自学 ★★★☆ 科研工作 ★★★ 坚持运动 ★★","text":"复习累了，码码字放松一下。大三下学期是个充满机遇和选择的学期，不断地纠结，争取，也获得了许多机会。学期快要结束，一个个努力的结果也渐渐到来。 寻找实习 ★★★★★ 保研学校 ★★★★★ 算法竞赛 ★★☆ 课余自学 ★★★☆ 科研工作 ★★★ 坚持运动 ★★ 学期刚开始的时候首先的想法就是要找一个心仪的实习。几个月来参加了各种笔试面试电面，发送了海量关于求职和询问的邮件，也拒绝过一些不错的公司。然而好事多磨，最想去的实习部门还没有拿到offer，也就不多说了。过几天等到有确定消息再来这儿更新。 不久前的我还是很纠结毕业后是要读研还是工作。在我交的实验室的所谓“科研经历”让我还是更加向往工作一些。然而本科毕业如果想从事机器学习和数据挖掘方面的工作，却的确显得太菜了。机缘巧合让我了解到南大的LAMDA实验室，经历了一波三折，最终过了LAMDA的面试。我想这里将会是一个能学到本事的地方，因此也让我下定决心选择读研。我在学分成绩上的短板曾一度让我不太敢想保研外校，能获得这样的结果的确是我天大的幸运。只等9月份的保研名额不要出意外就好了。 这个学期又有很多的算法比赛。Google Codejam，微软编程之美，还有ACM区域赛。在线比赛也没做出什么比较优异的成绩，去区域赛的机会也更多地让给大一大二的后生们了（笑）。距离最牛逼的人，还是有很大差距。下学期时间可能也会很紧，不知道会不会再去现场赛；学弟们的队伍也有几支实力比较牛的，希望我交的ACM能振兴。 学期初的时候保持着良好的势头，继续完成了Coursera上的课程。中后期各种事情繁忙，自学的脚步就放慢了许多。买了两本书《数学之美》和《集体智慧编程》，虽然还没读完，感觉内容不是很深入，学到的东西也很有限(可能书的定位就是如此）。接下来打算研读一下PRML，系统的学习一下机器学习这方面的知识。数日之前重新浏览了一遍线性代数（David C. Lay的经典教材），感觉还是略有一点收获。有机会要把各门数学课程，包括离散、高数、形式语言等到重新温习一下，“温故而知新”和“读书百遍，其义自现”还是很有道理的名言。 长达约一年的ITP项目，也就是“科研”，也快要结束了。有一点点成果，但我觉得还不能到达值得一提的程度。此处略去。 坚持运动也是个先前重要的打算，然而并没能坚持到最后。惭愧，考完试后继续。 总结一下，这个学期我收获了满意的实习机会和读研机会，觉得自己蛮幸运的。然而这只是个入场券，具体能从中得到什么样的成长，还要等半月后开始的实习，和一年后开始的研究生生涯中的努力来给自己一个交代；“时间会给你答案”。清楚自己要什么，认真做好该做的事情的日子，每周都能发现自己以肉眼可见的速度在成长。 思绪混乱，语无伦次，先写到这里。过几天有空再改，到时候再写一下南大面试的经历和对学校的一些期待。 2015-07-03 update距离写前面这些话又过去了三周，回到家也已经呆了一个星期。实习的安排已经定下来，拿到了梦寐以求的百度大数据实验室的offer，然而比原定的实习时间晚了两周。可以在家多点时间陪陪家人，下周可以回中学看看老师；不过开学之后的麻烦事儿可能会有些难处理。这次的实习机会可谓一波三折，折磨了我整整两个月，但我相信这都是值得的。 17号要去南大参加计算机系的统一考核，包括机试和面试。只要拿出几天复习一下，应该不是大问题。黎铭老师给我指定了 Machine Learning, Pattern Classification 和 Pattern Recognition and Machine Learning 三本书，现在只看完半本，压力着实有点大。深感自己英文的阅读速度还是太慢了，不过还是要一步一步来，慢慢努力。 祝我好运。","categories":[{"name":"Diary","slug":"Diary","permalink":"http://tedxz.github.io/categories/Diary/"}],"tags":[{"name":"Diary","slug":"Diary","permalink":"http://tedxz.github.io/tags/Diary/"},{"name":"Personal Review","slug":"Personal-Review","permalink":"http://tedxz.github.io/tags/Personal-Review/"}]},{"title":"DMLAB - 基于Logistic Regression的图像分类","slug":"dmlab-image-classification","date":"2015-05-12T07:19:02.000Z","updated":"2016-07-26T15:28:13.071Z","comments":true,"path":"2015/05/12/dmlab-image-classification/","link":"","permalink":"http://tedxz.github.io/2015/05/12/dmlab-image-classification/","excerpt":"一些题外话数据挖掘课内实验（大作业）做了一个简单的图像分类器，答辩受到老师的好评还蛮开心的；虽然很简单，但是是第一个自己完成的机器学习系统。今天内部消息看到自己数据挖掘的考试和答辩成绩都挺不错的，写个博文记录一下。 昨天听了一下午的“2015网络与数据科学研讨会”，和晚上的“百度-西安交大大数据竞赛”开幕式；讲的内容还是很激动人心的。开幕式后百度大数据实验室的夏粉老师对我也简短的面试了一下（BDL实习生），也聊到了这个简单的系统。","text":"一些题外话数据挖掘课内实验（大作业）做了一个简单的图像分类器，答辩受到老师的好评还蛮开心的；虽然很简单，但是是第一个自己完成的机器学习系统。今天内部消息看到自己数据挖掘的考试和答辩成绩都挺不错的，写个博文记录一下。 昨天听了一下午的“2015网络与数据科学研讨会”，和晚上的“百度-西安交大大数据竞赛”开幕式；讲的内容还是很激动人心的。开幕式后百度大数据实验室的夏粉老师对我也简短的面试了一下（BDL实习生），也聊到了这个简单的系统。 材料由于精力有限，就不在这里另写长篇的介绍了。把代码、答辩用的PPT和答辩稿（虽然最后没看稿）放在这里以供日后参考。 $(\".fancybox1\").fancybox({ openEffect : 'none', closeEffect : 'none', width : 960, height : 710, aspectRatio : true, scrolling : 'no' iframe : { preload: false } }); Slides &nbsp;&nbsp;&nbsp; Slides-PDF &nbsp;&nbsp;&nbsp; Speech &nbsp;&nbsp;&nbsp; Snapshots & Figures $(document).ready(function() { $(\".fancybox-button\").fancybox({ prevEffect : 'none', nextEffect : 'none', closeBtn : false, helpers : { title : { type : 'inside' }, buttons : {} } }); $(\".fancybox-snap\").fancybox({ prevEffect : 'none', nextEffect : 'none', closeBtn : false, helpers : { title : { type : 'inside' }, buttons : {} } }); }); 鸣谢感谢陈雪璐同学指正我slides中的bug！（Update 2015-5-14）","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://tedxz.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://tedxz.github.io/tags/Machine-Learning/"},{"name":"Logistic Regression","slug":"Logistic-Regression","permalink":"http://tedxz.github.io/tags/Logistic-Regression/"},{"name":"Data Minning","slug":"Data-Minning","permalink":"http://tedxz.github.io/tags/Data-Minning/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"http://tedxz.github.io/tags/Image-Classification/"}]},{"title":"使用非静态成员函数作为std::sort的比较函数","slug":"std-sort-with-non-static-method","date":"2015-05-12T06:50:13.000Z","updated":"2015-07-07T06:57:05.356Z","comments":true,"path":"2015/05/12/std-sort-with-non-static-method/","link":"","permalink":"http://tedxz.github.io/2015/05/12/std-sort-with-non-static-method/","excerpt":"前两天写FP树的时候，遇到了这个问题。函数需要根据对象内部的成员决定排序结果，因此也无法把成员函数修改成静态。查找资料后找到了一个巧妙的解决方案。该方案是使用一个内部结构体比较器持有外部类的引用，并将比较函数以重载结构体括号运算符的形式实现。 参考代码请read more。","text":"前两天写FP树的时候，遇到了这个问题。函数需要根据对象内部的成员决定排序结果，因此也无法把成员函数修改成静态。查找资料后找到了一个巧妙的解决方案。该方案是使用一个内部结构体比较器持有外部类的引用，并将比较函数以重载结构体括号运算符的形式实现。 参考代码请read more。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include &lt;cstdio&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;using namespace std;class Foo &#123;private: bool asc; struct Comparator &#123; Foo *foo; bool operator () (int a, int b) &#123; if (foo-&gt;asc) return a &lt; b; return a &gt; b; &#125; &#125;comparator; public: Foo() &#123; asc = true; comparator.foo = this; &#125; void setOrder(bool o) &#123; asc = o; &#125; void mySort(vector&lt;int&gt; *v) &#123; sort(v-&gt;begin(), v-&gt;end(), comparator); &#125;&#125;;int main() &#123; int a[] = &#123;1, 9, 2, 8, 3, 7, 4, 6, 5&#125;; vector&lt;int&gt; v(a, a + 9); Foo f; f.mySort(&amp;v); for (auto i = v.begin(); i != v.end(); ++i) cout &lt;&lt; *i &lt;&lt; \" \"; cout &lt;&lt; endl; f.setOrder(false); f.mySort(&amp;v); for (auto i = v.begin(); i != v.end(); ++i) cout &lt;&lt; *i &lt;&lt; \" \"; cout &lt;&lt; endl; f.setOrder(false); return 0;&#125; Reference: http://tipsandtricks.runicsoft.com/Cpp/Sort.html","categories":[{"name":"Skill","slug":"Skill","permalink":"http://tedxz.github.io/categories/Skill/"}],"tags":[{"name":"Skill","slug":"Skill","permalink":"http://tedxz.github.io/tags/Skill/"},{"name":"C++","slug":"C","permalink":"http://tedxz.github.io/tags/C/"},{"name":"STL","slug":"STL","permalink":"http://tedxz.github.io/tags/STL/"}]},{"title":"Google Code Jam 2015 - Round 1C","slug":"gcj-2015-r1c","date":"2015-05-11T14:41:53.000Z","updated":"2016-07-27T10:02:58.480Z","comments":true,"path":"2015/05/11/gcj-2015-r1c/","link":"","permalink":"http://tedxz.github.io/2015/05/11/gcj-2015-r1c/","excerpt":"又到了观众朋友们（其实根本没有观众）最喜爱的GCJ题解时间，本次给大家带来的是Round 1第三轮的题解。今天跟往常一样给大家带来3道题目： A. Brattleship B. Typewriter Monkey C. Less Money, More Problems 把Battleship拼成Brattleship是Google干的！我只是保持原样！","text":"又到了观众朋友们（其实根本没有观众）最喜爱的GCJ题解时间，本次给大家带来的是Round 1第三轮的题解。今天跟往常一样给大家带来3道题目： A. Brattleship B. Typewriter Monkey C. Less Money, More Problems 把Battleship拼成Brattleship是Google干的！我只是保持原样！ A. BrattleshipYou&apos;re about to play a simplified &quot;battleship&quot; game with your little brother. The board for this game is a rectangular grid with R rows and C columns. At the start of the game, you will close your eyes, and you will keep them closed until the end of the game. Your little brother will take a single rectangular 1 x W ship and place it horizontally somewhere on the board. The ship must always fit entirely on the board, with each cell of the ship occupying exactly one of the grid&apos;s cells, and it can never be rotated. In each turn of the game, you name a cell on the board, and your little brother tells you whether that is a hit (one of the cells occupied by the ship) or a miss. (Your little brother doesn&apos;t say which part of the ship was hit – just that the cell you named has a part of the ship in it.) You have perfect memory, and can keep track of all the information he has given you. Once you have named all of the cells occupied by the ship, the game is over (the ship is sunk), and your score is the number of turns taken. Your goal is to minimize your score. Although the ship is not supposed to be moved once it is placed, you know that your little brother, who is a brat, plans to cheat by changing the location of the ship whenever he wants, as long as the ship remains horizontal and completely on the board, and the new location is consistent with all the information he has given so far. For example, for a 1x4 board and 1x2 ship, your little brother could initially place the ship such that it overlaps the leftmost two columns. If your first guess was row 1, column 2, he could choose to secretly move the ship to the rightmost two columns, and tell you that (1, 2) was a miss. If your next guess after that was (1, 3), though, then he could not say that was also a miss and move the ship back to its original location, since that would be inconsistent with what he said about (1, 2) earlier. Not only do you know that your little brother will cheat, he knows that you know. If you both play optimally (you to minimize your score, him to maximize it), what is the lowest score that you can guarantee you will achieve, regardless of what your little brother does? Input The first line of the input gives the number of test cases, T. T lines follow, each with three space-separated integers R, C, and W: the number of rows and columns of the board, followed by the width of the ship. Output For each test case, output one line containing &quot;Case #x: y&quot;, where x is the test case number (starting from 1) and y is the minimum score you can guarantee. Limits 1 ≤ W ≤ C. Small dataset T = 55.R = 1.1 ≤ C ≤ 10. Large dataset 1 ≤ T ≤ 100.1 ≤ R ≤ 20.1 ≤ C ≤ 20. A. Solution相信大家都玩过类似的游戏，这个题目是其简化版。 题目大意：你和你的小哥哥（误）（我就不翻译成弟弟）玩游戏，他在一个RC的棋盘上水平放置一个1W的战船，但是你并不能直接看到这艘船。每个回合，你可以向棋盘上某个格子开炮，并可以知道是否击中。你的目标是用最短的回合数将战舰所占的格子全部炮击以将其击沉。然而小哥哥比较狡猾，在不影响你已经炮击的格子的结果的情况下，他会尽量让你完成任务的回合数增长。试问双方在各自的最优规则下，你需要多少回合击沉战舰。 这个问题总的来说并不复杂，但是在某些情况下并不知道双方的最优策略是什么；或者说想出来双方的最优策略还需要证明其最优性。 很显然，由于你的小哥哥太坑爹，我们在进行游戏的时候首先应该将棋盘的(R-1)行排除战舰藏匿的可能（坑爹的小哥哥不会让你在超过一行可能藏匿战舰的情况下击中战舰），然后在最后剩下的一行中将战舰逼迫现身。由于题目中说明战舰一定是水平放置，将一行排除只要每间隔(W-1)个格子炮击一次即可。扫清(R-1)行只需要炮击(R-1)*floor(C/W)次。 剩下一行连续的C个格子中有连续的W个格子中有战船。经过考虑我发现可以将问题以以下的几种情况概况，并一步一步将问题规模减小： W=1。这个时候，我们必须将C个格子逐一炮击才能将战舰完全击沉。答案累计C并结束。 C≥2W。这个时候，我们下一炮一定无法击中战舰，因此只能最大程度上缩小战舰所在的范围。在连续的C个格子一端起第W个格子处炮击是最优的做法之一，答案累计1，C=C-W。 C&lt;2W。这个时候，猛击C个格子的中间格子，必定能击中战舰。这里有一个推论：如果能击中战舰，优先打击确定能击中的格子；因为你早晚要打这一炮。答案累计1，C=C-1，W=W-1。 由于问题规模不大，这个算法也无需实现成记忆话搜索。问题得到解决。 B. Typewriter MonkeyYour publishing house has decided to use monkeys randomly typing at keyboards to write great works of literature. You are the supervisor for one monkey with a keyboard containing K keys, each of which is labeled with an uppercase English letter. (There may be multiple keys displaying the same letter.) The monkey will start with an empty string and repeat the following S times: choose a key from its keyboard uniformly at random and press it, adding a copy of that key&apos;s letter to the right end of the string. The final resulting string will have length S. You have a target word of length L that you are hoping the monkey will type. (The target word will not necessarily be a real English word.) This target word may even appear multiple times in what the monkey types. (Overlapping instances count too – for example, if &quot;ABA&quot; is the target word and the monkey types &quot;ABABA&quot;, that contains two instances of the target.) You plan to pay the monkey one banana for each instance of the target word that it types. When you go to inspect the monkey&apos;s work, you will bring along the minimum number of bananas that you need to ensure that you will always have enough bananas to pay the monkey, no matter what it has typed. Then, you will pay the monkey one banana for each instance of the target word that it actually typed. You will keep the remaining bananas that you brought with you. What is the expected number of bananas that you will get to keep? Input The first line of the input gives the number of test cases, T. T test cases follow. Each consists of three lines. The first contains three space-separated positive integers: K, L, and S. The second contains a string of K uppercase English letters representing the monkey&apos;s keyboard. The third contains a string of L uppercase English letters representing the target word. Output For each test case, output one line containing &quot;Case #x: y&quot;, where y is the expected number of bananas you will get to keep after paying the monkey. y will be considered correct if it is within an absolute or relative error of 10-6 of the correct answer. See the FAQ for an explanation of what that means, and what formats of real numbers we accept. Limits 1 ≤ T ≤ 100. Small dataset 1 ≤ K ≤ 7.1 ≤ L ≤ S ≤ 7. Large dataset 1 ≤ K ≤ 100.1 ≤ L ≤ S ≤ 100. B. Solution 猴子和打字机（Monkeys and Typewriters），如果无数多的猴子在无数多的打字机上随机的打字，并持续无限久的时间，那么在某个时候，它们必然会打出莎士比亚的全部著作。猴子和打字机的设想在20世纪初被法国数学家Emile Borel推广，但其基本思想——无数多的人员和无数多的时间能产生任何/所有东西——可以追溯至亚里士多德。 题目大意：在一个给定的键盘布局上，猴子随机的敲S个键，每碰巧敲出一个keyword（允许重叠），你就要奖励他一个香蕉。你带来了满足在任何情况下都能足够奖励猴子的个数的香蕉（并且尽量少），但是大多数情况下这些香蕉并不能全部奖励出去。你要计算的是你剩下香蕉的期望个数。 这个问题分为两个部分：求出最多可能出现的keyword出现次数（要带多少香蕉）和keyword出现次数的期望（给出的香蕉期望）。 第一个问题比较好解决：算一下关键字最少偏移多少距离可以自重合，然后算最多出现次数了。不多说，了解KMP的都懂。 第二个问题：因为每个位置出现关键字互不影响；我们计算所有情况下在每个位置会出现的关键字次数；除以总的可能出现的字符串个数记得到keyword出现的期望次数。 不考虑键盘有重复按键的话，在特定位置出现关键字的不同情况一共是K^(S-L)。由于有(S-L+1)个不同的起点，两者相乘。由于按键有可能重复，对于keyword中的每个字母，乘以这个字符在键盘中出现的次数即可（记为multiplier）。 最终期望为：multiplier*K^(S-L)*(S-L+1)/(K^S)，约分得multiplier*(S-L+1)/(K^L)。 另外需要特判一下键盘根本打不出来这个关键字的情况。 C. Less Money, More ProblemsUp until today, the nation you live in has used D different positive integer denominations of coin for all transactions. Today, the queen got angry when a subject tried to pay his taxes with a giant sack of low-valued coins, and she just decreed that no more than C coins of any one denomination may be used in any one purchase. For instance, if C = 2 and the existing denominations are 1 and 5, it is possible to buy something of value 11 by using two 5s and one 1, or something of value 12 by using two 5s and two 1s, but it is impossible to buy something of value 9 or 17. You cannot directly challenge the queen&apos;s decree, but you happen to be in charge of the mint, and you can issue new denominations of coin. You want to make it possible for any item of positive value at most V to be purchased under the queen&apos;s new rules. (Note that this may not necessarily have been possible before the queen&apos;s decree.) Moreover, you want to introduce as few new denominations as possible, and your final combined set of pre-existing and new denominations may not have any repeats. What is the smallest number of new denominations required? Input The first line of the input gives the number of test cases, T. T test cases follow. Each consists of one line with three space-separated values C, D, and V, followed by another line with D distinct space-separated values representing the preexisting denominations, in ascending order. Output For each test case, output one line containing &quot;Case #x: y&quot;, where x is the test case number (starting from 1) and y is the minimum number of new denominations required, as described above. Limits 1 ≤ T ≤ 100.Each existing denomination ≤ V. Small dataset C = 1.1 ≤ D ≤ 5.1 ≤ V ≤ 30. Large dataset 1 ≤ C ≤ 100.1 ≤ D ≤ 100.1 ≤ V ≤ 109. C.Solution题目大意：女王下令在每一次支付活动中，同一种面值的货币不能使用超过C个。你作为发行货币的官员，希望在满足女王懿旨的情况下增加货币的面值，使得不超过V元的交易都能完成。目前已有D种不同面值的货币，请问最少增加多少种不同面值的货币？ 设货币按面值排序，第1~(i-1)中货币应该能凑出来0~(d[i]-1)中所有的额度。 其中di是第i小的货币的面值。如果不能，设前(i-1)种货币最多能凑出1~maxv[i-1]的所有面值，那么我们应该增发面值为d&apos;=maxv[i-1]+1的货币。d&apos;缩小将缩小货币组合面值的能力，增大则不能连续组合面值。 因此从小到大维护前i种面值能连续组合的最大面值。对于每次循环取出一个面值更大的货币：如果d[i]&gt;maxv[i-1]+1，那么添加一个面值为d&apos;=maxv[i-1]+1的货币，此时连续能够组合的面值为maxv=maxv+d&apos;*c；否则继续取下一个已有的面值，maxv=maxv+d[i]*c。","categories":[{"name":"Algorithm Contest","slug":"Algorithm-Contest","permalink":"http://tedxz.github.io/categories/Algorithm-Contest/"}],"tags":[{"name":"Algorithm Contest","slug":"Algorithm-Contest","permalink":"http://tedxz.github.io/tags/Algorithm-Contest/"},{"name":"Google Codejam","slug":"Google-Codejam","permalink":"http://tedxz.github.io/tags/Google-Codejam/"}]},{"title":"按照1的个数枚举数字","slug":"enumerate-nums-by-1-bits","date":"2015-05-03T08:57:10.000Z","updated":"2015-05-03T09:05:57.785Z","comments":true,"path":"2015/05/03/enumerate-nums-by-1-bits/","link":"","permalink":"http://tedxz.github.io/2015/05/03/enumerate-nums-by-1-bits/","excerpt":"","text":"在产生n中选k的不同方案的时候，这段代码就很有用了。12345678for (int k = 1; k &lt;= n; k++) &#123; for (int s = (1 &lt;&lt; k) - 1, u = 1 &lt;&lt; n; s &lt; u;) &#123; for (int i = 0; i &lt; n; i++) printf(\"%d\\n\", s &gt;&gt; (n - 1 - i) &amp; 1); int b = s &amp; -s; s = (s + b) | (((s ^ (s + b)) &gt;&gt; 2) / b); &#125;&#125; 其中s &amp; -s是lowbit。","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://tedxz.github.io/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://tedxz.github.io/tags/Algorithm/"}]},{"title":"Google Code Jam 2015 - Round 1B","slug":"gcj-2015-r1b","date":"2015-05-03T03:32:38.000Z","updated":"2016-07-26T16:40:11.591Z","comments":true,"path":"2015/05/03/gcj-2015-r1b/","link":"","permalink":"http://tedxz.github.io/2015/05/03/gcj-2015-r1b/","excerpt":"上一场已经晋级的我昨晚陪华哥打第二场，比赛从晚上12点开始持续两个半小时，果断又不能早睡了。这次的题目不是很对胃口（笑），华哥手滑未能晋级，一周后我们又可以开心地打第三场了。 像上次一样，这次共有三道题： A. Counter Culture B. Noisy Neighbors C. Hiking Deer","text":"上一场已经晋级的我昨晚陪华哥打第二场，比赛从晚上12点开始持续两个半小时，果断又不能早睡了。这次的题目不是很对胃口（笑），华哥手滑未能晋级，一周后我们又可以开心地打第三场了。 像上次一样，这次共有三道题： A. Counter Culture B. Noisy Neighbors C. Hiking Deer A. Counter CultureIn the Counting Poetry Slam, a performer takes the microphone, chooses a number N, and counts aloud from 1 to N. That is, she starts by saying 1, and then repeatedly says the number that is 1 greater than the previous number she said, stopping after she has said N. It’s your turn to perform, but you find this process tedious, and you want to add a twist to speed it up: sometimes, instead of adding 1 to the previous number, you might reverse the digits of the number (removing any leading zeroes that this creates). For example, after saying “16”, you could next say either “17” or “61”; after saying “2300”, you could next say either “2301” or “32”. You may reverse as many times as you want (or not at all) within a performance. The first number you say must be 1; what is the fewest number of numbers you will need to say in order to reach the number N? 1 and N count toward this total. If you say the same number multiple times, each of those times counts separately. Input The first line of the input gives the number of test cases, T. T lines follow. Each has one integer N, the number you must reach. Output For each test case, output one line containing “Case #x: y”, where x is the test case number (starting from 1) and y is the minimum number of numbers you need to say. Limits 1 ≤ T ≤ 100. Small dataset 1 ≤ N ≤ 106. Large dataset 1 ≤ N ≤ 1014. A. Solution题目大意：从1开始报数，每次可以在刚刚报过数字的基础上加一，或者将刚刚报过的数字翻转。问最少几步可以报到给定的数字。 小数据的范围106，可以用O(n)时间复杂度的算法暴力求解。 正确的方法是进行BFS广度优先搜索。首先将1加入队列，然后对于队列中的每一个数字扩展（判重）后加入队列。数据规模是106，因此对于超过这一范围的数字不再进行扩展。 前面我们提到，翻转有可能使数字从大变小，那么当给定数字n在范围106内，会不会出现先报过超过106的数字，再通过翻转变小呢？通过观察BFS扩展的情况我们发现，最短路径上的翻转扩展并不会使数字变小。 嗯，观察数据找规律的确是很重要的技巧。 将BFS过程中数字翻转进行的扩展情况输出，我们可以看到下面的内容： 12345678910111213141516171819202122232425262728293031...1074 -&gt; 47011075 -&gt; 57011076 -&gt; 67011077 -&gt; 77011078 -&gt; 87011079 -&gt; 97011081 -&gt; 18011082 -&gt; 28011083 -&gt; 38011084 -&gt; 48011085 -&gt; 58011086 -&gt; 68011087 -&gt; 78011088 -&gt; 88011089 -&gt; 98011091 -&gt; 19011092 -&gt; 29011093 -&gt; 39011094 -&gt; 49011095 -&gt; 59011096 -&gt; 69011097 -&gt; 79011098 -&gt; 89011099 -&gt; 990110002 -&gt; 2000110003 -&gt; 3000110004 -&gt; 4000110005 -&gt; 5000110006 -&gt; 60001... 通过观察BFS过程中的数字翻转扩展，我们发现最短路径上的翻转总是出现在数字的前半部分是1000..的情况。换句话说，我们从n开始倒推的时候，通过判断数字的形式就可以知道它是通过小数字加一得来还是翻转得来。如果数字的后一半是…0001(且该数字不是回文的形式），我们将其翻转，否则则是一步一步减小到后一半为…0001。这种方法可以迅速将数字缩小并累计得到答案。 B. Noisy NeighborsYou are a landlord who owns a building that is an R x C grid of apartments; each apartment is a unit square cell with four walls. You want to rent out N of these apartments to tenants, with exactly one tenant per apartment, and leave the others empty. Unfortunately, all of your potential tenants are noisy, so whenever any two occupied apartments share a wall (and not just a corner), this will add one point of unhappiness to the building. For example, a 2x2 building in which every apartment is occupied has four walls that are shared by neighboring tenants, and so the building’s unhappiness score is 4. If you place your N tenants optimally, what is the minimum unhappiness value for your building? Input The first line of the input gives the number of test cases, T. T lines follow; each contains three space-separated integers: R, C, and N. Output For each test case, output one line containing “Case #x: y”, where x is the test case number (starting from 1) and y is the minimum possible unhappiness for the building. Limits 1 ≤ T ≤ 1000.0 ≤ N ≤ R*C. Small dataset 1 ≤ R*C ≤ 16. Large dataset 1 ≤ R*C ≤ 10000. B. Solution题目大意：有R*C个网格状排列的房间，要将其中n个房间安排房客。由于噪音问题，每一对占用相邻房间（共用同一堵墙）的客人会对整个建筑累计1的不愉快度。给定R、C和n，求累积的不愉快度最小值。 对于小数据，我们可以暴力枚举每个房间是否住人，计算答案选择最小的即可。 对于大数据，我们有这样一个结论：所有的客人先按照国际象棋棋盘那样对角相邻安置（此时不愉快度为0），剩下的客人按照相邻房客较少的原则贪心地安排在剩下的房间中即可总不愉快度最小。我这里无法给出这个结论的证明，有了解的同学可以评论或联系我~ 按照棋盘装对角安置通常的做法是使用横纵坐标和的奇偶性把房间划分成两类，两类房间各自全部占满都不会有不愉快度。计算答案的时候应该分别计算优先安置奇数号房间和优先安置偶数号房间两种情况，选其中较优的。 C. Hiking DeerHerbert Hooves the deer is going for a hike: one clockwise loop around his favorite circular trail, starting at degree zero. Herbert has perfect control over his speed, which can be any nonnegative value (not necessarily an integer) at any time – he can change his speed instantaneously whenever he wants. When Herbert reaches his starting point again, the hike is over. The trail is also used by human hikers, who also walk clockwise around the trail. Each hiker has a starting point and moves at her own constant speed. Humans continue to walk around and around the trail forever. Herbert is a skittish deer who is afraid of people. He does not like to have encounters with hikers. An encounter occurs whenever Herbert and a hiker are in exactly the same place at the same time. You should consider Herbert and the hikers to be points on the circumference of a circle. Herbert can have multiple separate encounters with the same hiker. If more than one hiker is encountered at the same instant, all of them count as separate encounters. Any encounter at the exact instant that Herbert finishes his hike still counts as an encounter. If Herbert were to have an encounter with a hiker and then change his speed to exactly match that hiker’s speed and follow along, he would have infinitely many encounters! Of course, he must never do this. Encounters do not change the hikers’ behavior, and nothing happens when hikers encounter each other. Herbert knows the starting position and speed of each hiker. What is the minimum number of encounters with hikers that he can possibly have? Solving this problem Usually, Google Code Jam problems have 1 Small input and 1 Large input. This problem has 2 Small inputs and 1 Large input. You must solve the first Small input before you can attempt the second Small input; as usual, you will be able to retry the Small inputs (with a time penalty). Once you have solved both Small inputs, you will be able to download the Large input; as usual, you will get only one chance at the Large input. Input The first line of the input gives the number of test cases, T. T test cases follow. Each begins with one line with an integer N, and is followed by N lines, each of which represents a group of hikers starting at the same position on the trail. The ith of these lines has three space-separated integers: a starting position Di (representing Di/360ths of the way around the trail from the deer’s starting point), the number Hi of hikers in the group, and Mi, the amount of time (in minutes) it takes for the fastest hiker in that group to make each complete revolution around the circle. The other hikers in that group each complete a revolution in Mi+1, Mi+2, …, Mi+Hi-1 minutes. For example, the line 180 3 4 would mean that three hikers begin halfway around the trail from the deer’s starting point, and that they take 4, 5, and 6 minutes, respectively, to complete each full revolution around the trail. Herbert always starts at position 0 (0/360ths of the way around the circle), and no group of hikers does. Multiple groups of hikers may begin in the same place, but no two hikers will both begin in the same place and have the same speed. Output For each test case, output one line containing “Case #x: y”, where x is the test case number (starting from 1) and y is the minimum number of encounters with hikers that the deer can have. Limits 1 ≤ T ≤ 100.1 ≤ Di ≤ 359.1 ≤ N ≤ 1000.1 ≤ Hi.1 ≤ Mi ≤ 109. (Note that this only puts a cap on the time required for the fastest hiker in each group to complete a revolution. Slower hikers in the group will take longer.) C. Solution这个题目的确有些难度，以至于Google给了这个题目两组小数据一组大数据，分数分别是13、16、19。如果想不出标准算法，过小数据的程序需要判断很多边界情况，也很难实现。 题目大意：一直梅花鹿打算绕环形公路顺时针跑一圈，最初在相角为0的位置。同时公路上不同位置还有很多人也在顺时针跑步，每个人在初始时刻有一个位置（相角），速度恒定。梅花鹿不喜欢见人，希望在跑步过程中遭遇最少的人次。求最少可以遭遇的人次数。 我们画出跑步的人和鹿的时空图，其中红线和蓝线分别代表了两个跑步的人，虚线代表了梅花鹿。虚线与其他线的每一个交点即意味着一次相遇。图中梅花鹿是匀速运动的，而题目中强调梅花鹿的速度可以随时任意变换——但这并不会影响结果：对于一个确定的梅花鹿时刻和位置的末状态(phi,t)和初始状态(0,0)，如果这两个点在某条线段的一上一下，那么至少有一个交点，无论梅花鹿的轨迹是直线段或是曲线段。对于初始和结束点都重合的情况，我们可以不考虑，不会影响答案的正确性（因为可以微调终点使得相遇次数减少）。 假设一共有U个跑步者，那么答案一定不会超过U。梅花鹿以极快的速度超过所有人，遭遇每人一次，答案即为U。在时空图上即为一条很贴近角度轴的线段。 如何将答案进一步缩小呢？我们减慢速度，使得少超过一部分人。在时空图上的体现表现为梅花鹿的终状态沿着直线phi=360这条直线向上移动。终点每跨越一个线段，可能是原来超过的一个跑者变为了不超过这个跑者，此时应该将答案减小1；也可能是被一个跑者超过了（此时这个跑者之前一定先从被梅花鹿超过变成不被超过1次，然后赶超了梅花鹿0次或多次），此时答案应该增加1。 我们分别统计梅花鹿超过的人次和被人超过的次数，相加即为答案。初始的解是超过人次为n（每人一次），被超过0次。然后按照每个人到达360度位置的时间枚举梅花鹿跑完的时间。随着这一时间增加，超过的人次减少，被超过的次数增加。当被超过的次数达到U，我们就不可能得到更好的解了，此时可以退出并返回已经遇到的最优解。","categories":[{"name":"Algorithm Contest","slug":"Algorithm-Contest","permalink":"http://tedxz.github.io/categories/Algorithm-Contest/"}],"tags":[{"name":"Algorithm Contest","slug":"Algorithm-Contest","permalink":"http://tedxz.github.io/tags/Algorithm-Contest/"},{"name":"Google Codejam","slug":"Google-Codejam","permalink":"http://tedxz.github.io/tags/Google-Codejam/"}]},{"title":"Google Code Jam 2015 - Round 1A","slug":"gcj-2015-r1a","date":"2015-04-18T07:15:05.000Z","updated":"2016-07-26T16:39:52.625Z","comments":true,"path":"2015/04/18/gcj-2015-r1a/","link":"","permalink":"http://tedxz.github.io/2015/04/18/gcj-2015-r1a/","excerpt":"GCJ Round1的第一场比赛在今天上午九点开始，为此我还请假了实验室的例会。在这里就不吐槽不知道比赛时间起晚的同学了。 这次比赛时长两个半小时，共有三道题： A. Mushroom Monster B. Haircut C. Logging 这个时限对我来说不松不紧，如果再延长半小时或许能AK。希望有朝一日成为一只手速狗。","text":"GCJ Round1的第一场比赛在今天上午九点开始，为此我还请假了实验室的例会。在这里就不吐槽不知道比赛时间起晚的同学了。 这次比赛时长两个半小时，共有三道题： A. Mushroom Monster B. Haircut C. Logging 这个时限对我来说不松不紧，如果再延长半小时或许能AK。希望有朝一日成为一只手速狗。 A. Mushroom MonsterKaylin loves mushrooms. Put them on her plate and she&apos;ll eat them up! In this problem she&apos;s eating a plate of mushrooms, and Bartholomew is putting more pieces on her plate. In this problem, we&apos;ll look at how many pieces of mushroom are on her plate at 10-second intervals. Bartholomew could put any non-negative integer number of mushroom pieces down at any time, and the only way they can leave the plate is by being eaten. Figure out the minimum number of mushrooms that Kaylin could have eaten using two different methods of computation: Assume Kaylin could eat any number of mushroom pieces at any time.Assume that, starting with the first time we look at the plate, Kaylin eats mushrooms at a constant rate whenever there are mushrooms on her plate. For example, if the input is 10 5 15 5: With the first method, Kaylin must have eaten at least 15 mushroom pieces: first she eats 5, then 10 more are put on her plate, then she eats another 10. There&apos;s no way she could have eaten fewer pieces. With the second method, Kaylin must have eaten at least 25 mushroom pieces. We can determine that she must eat mushrooms at a rate of at least 1 piece per second. She starts with 10 pieces on her plate. In the first 10 seconds, she eats 10 pieces, and 5 more are put on her plate. In the next 5 seconds, she eats 5 pieces, then her plate stays empty for 5 seconds, and then Bartholomew puts 15 more pieces on her plate. Then she eats 10 pieces in the last 10 seconds. Input The first line of the input gives the number of test cases, T. T test cases follow. Each will consist of one line containing a single integer N, followed by a line containing N space-separated integers mi; the number of mushrooms on Kaylin&apos;s plate at the start, and at 10-second intervals. Output For each test case, output one line containing &quot;Case #x: y z&quot;, where x is the test case number (starting from 1), y is the minimum number of mushrooms Kaylin could have eaten using the first method of computation, and z is the minimum number of mushrooms Kaylin could have eaten using the second method of computation. Limits 1 ≤ T ≤ 100. Small dataset 2 ≤ N ≤ 10.0 ≤ mi ≤ 100. Large dataset 2 ≤ N ≤ 1000.0 ≤ mi ≤ 10000. A. Solution这道题的题意有些含混，我读了好久才读懂。 题目大意：Kaylin负责吃盘子里的蘑菇，Bartholomew负责往盘子中放置蘑菇。输入数据是从0时刻开始每隔10秒钟记录下来的盘子中的蘑菇数量。假设Kaylin有两种吃蘑菇的方式，我们要求的是Kaylin用这两种方式吃蘑菇的话，最少吃了多少个蘑菇，使得记录的蘑菇数量有可能成立。 第一种方式是Kaylin可以在她想吃蘑菇的时候瞬间吃掉若干个蘑菇。 第二种方式是Kaylin以固定的速率吃蘑菇。 Bartholomew可能会在任意时刻放置任意数量的蘑菇。 了解了题意，这道题目就很容易做了。在第一种方式下，我们认为相邻10秒钟的蘑菇数量如果增加，便是Bartholomew放置了相差数量的蘑菇而Kaylin没有吃，如果减小则是Kaylin吃了相差数量的蘑菇而Bartholomew没有放。累加答案即可。 在第二种方式下，我们显然知道为了满足吃了蘑菇最少的条件，对于固定长度的时间，Kaylin吃的越慢越好。因此Kaylin吃蘑菇的速度应该是相邻10秒钟蘑菇减小最多的减小数量（每10秒）。知道了速度还不能直接乘以总时间求吃掉的蘑菇数量，因为有可能在某个10秒区间开始时盘子里已经没有足够她10秒吃的蘑菇了，此时假设在这10秒内先吃光了剩下的蘑菇，在这个10秒区间结束时Bartholomew又放入了若干蘑菇，可以使得Kaylin吃掉的蘑菇数量最少。 B. HaircutYou are waiting in a long line to get a haircut at a trendy barber shop. The shop has B barbers on duty, and they are numbered 1 through B. It always takes the kth barber exactly Mk minutes to cut a customer&apos;s hair, and a barber can only cut one customer&apos;s hair at a time. Once a barber finishes cutting hair, he is immediately free to help another customer. While the shop is open, the customer at the head of the queue always goes to the lowest-numbered barber who is available. When no barber is available, that customer waits until at least one becomes available. You are the Nth person in line, and the shop has just opened. Which barber will cut your hair? Input The first line of the input gives the number of test cases, T. T test cases follow; each consists of two lines. The first contains two space-separated integers B and N – the number of barbers and your place in line. The customer at the head of the line is number 1, the next one is number 2, and so on. The second line contains M1, M2, …, MB. Output For each test case, output one line containing &quot;Case #x: y&quot;, where x is the test case number (starting from 1) and y is the number of the barber who will cut your hair. Limits 1 ≤ T ≤ 100.1 ≤ N ≤ 109. Small dataset 1 ≤ B ≤ 5.1 ≤ Mk ≤ 25. Large dataset 1 ≤ B ≤ 1000.1 ≤ Mk ≤ 100000. B. Solution第二题虽然不是难题，但是写起来很难考虑。 题目大意：理发店里有b个理发师，第i个理发师给一位客人理发需要${M_i}$分钟。每当有理发师空闲，他就为正在等待的下一位客人服务，如果同时有两个以上理发师空闲，客人会被编号较小的理发师服务。我们要算的是排在第n位的客人会被哪一位理发师服务。 由于n的范围比较大，暴力模拟显然是不可取的。我们可以先二分得到能够让第n个客人开始理发的最少分钟数，然后判断他是哪位理发师服务的。要注意的是可能有与第n位客人同时开始理发的客人，这些客人编号从小到大依次与空闲的理发师按顺序匹配。 C. LoggingA certain forest consists of N trees, each of which is inhabited by a squirrel. The boundary of the forest is the convex polygon of smallest area which contains every tree, as if a giant rubber band had been stretched around the outside of the forest. Formally, every tree is a single point in two-dimensional space with unique coordinates (Xi, Yi), and the boundary is the convex hull of those points. Some trees are on the boundary of the forest, which means they are on an edge or a corner of the polygon. The squirrels wonder how close their trees are to being on the boundary of the forest. One at a time, each squirrel climbs down from its tree, examines the forest, and determines the minimum number of trees that would need to be cut down for its own tree to be on the boundary. It then writes that number down on a log. Determine the list of numbers written on the log. Input The first line of the input gives the number of test cases, T. T test cases follow; each consists of a single line with an integer N, the number of trees, followed by N lines with two space-separated integers Xi and Yi, the coordinates of each tree. No two trees will have the same coordinates. Output For each test case, output one line containing &quot;Case #x:&quot;, followed by N lines with one integer each, where line i contains the number of trees that the squirrel living in tree i would need to cut down. Limits -106 ≤ Xi, Yi ≤ 106. Small dataset 1 ≤ T ≤ 100.1 ≤ N ≤ 15. Large dataset 1 ≤ T ≤ 14.1 ≤ N ≤ 3000. C. Solution这道题有关凸包，但是实际上并不需要求出凸包。对于我这个不常写计算几何题的人来说，这道题里面只用到了简单的叉积，并不麻烦。 题目大意：平面上有n个点，对于每一个点，我们要求出来去掉几个其他点之后，该点会落在这些点围成的凸包边界上。 对于凸包上的任意一条边所在的直线，整个凸包一定在这条直线的一侧（即不会跨越这条直线）。对于某个点$P_i$，我们可以枚举过该点的所有直线，这样整个平面被分成了两边。去掉某一边的所有点之后（当然是点数较少的那边，不包括线上），$P_i$就落在了新点集的凸包上。如何枚举直线？只需枚举过$P_i$和另一点$P_j$的有限条即可。 比赛中我想到的想法是，枚举了待研究的点和过该点的直线后，对于其他所有点判断在直线的哪一边。之后取较少的一边用于更新答案。整个算法的复杂度$O(n^3)$。判断在直线的哪一侧只需计算两个向量的叉积的符号（等于0表示在线上，不用计数）。这个方法可以过掉小数据，但是对于n高达3000的大数据有些吃力。加入少许最优化剪枝之后我尝试跑了一下大数据，在8分钟之内将14个测试数据跑出来12个（就差一点点，遗憾）。 比赛过程中贡献了一次Wrong Answer，是因为n=1的极端情况。另外要处理的是枚举直线时i≠j，否则直线的方向向量是0，与任何向量做叉积，结果都是0。 显然这个算法可以进一步优化使得对于大数据也能在一秒钟内出解。每次重新计算所有点落在直线的哪一侧实际上冗余非常大。我们可以在选取了基准点之后，将其他的点按照极角排序，然后再排序之后的点中$O(n)$地扫描枚举角度相差180°的区间。排序时间复杂度是$O(nlogn)$，总时间复杂度$O(n^2 logn)$。","categories":[{"name":"Algorithm Contest","slug":"Algorithm-Contest","permalink":"http://tedxz.github.io/categories/Algorithm-Contest/"}],"tags":[{"name":"Algorithm Contest","slug":"Algorithm-Contest","permalink":"http://tedxz.github.io/tags/Algorithm-Contest/"},{"name":"Google Codejam","slug":"Google-Codejam","permalink":"http://tedxz.github.io/tags/Google-Codejam/"}]},{"title":"Google Code Jam 2015 - Qualification Round","slug":"gcj-2015-qr","date":"2015-04-11T13:42:38.000Z","updated":"2016-07-26T16:39:30.646Z","comments":true,"path":"2015/04/11/gcj-2015-qr/","link":"","permalink":"http://tedxz.github.io/2015/04/11/gcj-2015-qr/","excerpt":"又到了比赛季，有了博客终于可以好好的写个题解。今天是GCJ的资格赛，题目还好比较简单。 这次的比赛有四道题： A. Standing Ovation B. Infinite House of Pancakes C. Dijkstra D. Ominous Omino 为了防止剧透，预览线靠前一点，详情请点击~","text":"又到了比赛季，有了博客终于可以好好的写个题解。今天是GCJ的资格赛，题目还好比较简单。 这次的比赛有四道题： A. Standing Ovation B. Infinite House of Pancakes C. Dijkstra D. Ominous Omino 为了防止剧透，预览线靠前一点，详情请点击~ A. Standing OvatioIt’s opening night at the opera, and your friend is the prima donna (the lead female singer). You will not be in the audience, but you want to make sure she receives a standing ovation – with every audience member standing up and clapping their hands for her. Initially, the entire audience is seated. Everyone in the audience has a shyness level. An audience member with shyness level Si will wait until at least Si other audience members have already stood up to clap, and if so, she will immediately stand up and clap. If Si = 0, then the audience member will always stand up and clap immediately, regardless of what anyone else does. For example, an audience member with Si = 2 will be seated at the beginning, but will stand up to clap later after she sees at least two other people standing and clapping. You know the shyness level of everyone in the audience, and you are prepared to invite additional friends of the prima donna to be in the audience to ensure that everyone in the crowd stands up and claps in the end. Each of these friends may have any shyness value that you wish, not necessarily the same. What is the minimum number of friends that you need to invite to guarantee a standing ovation? Input The first line of the input gives the number of test cases, T. T test cases follow. Each consists of one line with Smax, the maximum shyness level of the shyest person in the audience, followed by a string of Smax + 1 single digits. The kth digit of this string (counting starting from 0) represents how many people in the audience have shyness level k. For example, the string “409” would mean that there were four audience members with Si = 0 and nine audience members with Si = 2 (and none with Si = 1 or any other value). Note that there will initially always be between 0 and 9 people with each shyness level. The string will never end in a 0. Note that this implies that there will always be at least one person in the audience. Output For each test case, output one line containing “Case #x: y”, where x is the test case number (starting from 1) and y is the minimum number of friends you must invite. Limits 1 ≤ T ≤ 100. Small dataset 0 ≤ Smax ≤ 6. Large dataset 0 ≤ Smax ≤ 1000. A. Solution这道题比较简单，以至于A掉这道题都不能过资格赛。保证S值低于i的观众有i个即可，扫一遍累计前缀和，不够的时候补上就行了。 B. Infinite House of PancakesAt the Infinite House of Pancakes, there are only finitely many pancakes, but there are infinitely many diners who would be willing to eat them! When the restaurant opens for breakfast, among the infinitely many diners, exactly D have non-empty plates; the ith of these has Pi pancakes on his or her plate. Everyone else has an empty plate. Normally, every minute, every diner with a non-empty plate will eat one pancake from his or her plate. However, some minutes may be special. In a special minute, the head server asks for the diners’ attention, chooses a diner with a non-empty plate, and carefully lifts some number of pancakes off of that diner’s plate and moves those pancakes onto one other diner’s (empty or non-empty) plate. No diners eat during a special minute, because it would be rude. You are the head server on duty this morning, and it is your job to decide which minutes, if any, will be special, and which pancakes will move where. That is, every minute, you can decide to either do nothing and let the diners eat, or declare a special minute and interrupt the diners to make a single movement of one or more pancakes, as described above. Breakfast ends when there are no more pancakes left to eat. How quickly can you make that happen? Input The first line of the input gives the number of test cases, T. T test cases follow. Each consists of one line with D, the number of diners with non-empty plates, followed by another line with D space-separated integers representing the numbers of pancakes on those diners’ plates. Output For each test case, output one line containing “Case #x: y”, where x is the test case number (starting from 1) and y is the smallest number of minutes needed to finish the breakfast. Limits 1 ≤ T ≤ 100. Small dataset 1 ≤ D ≤ 6.1 ≤ Pi ≤ 9. Large dataset 1 ≤ D ≤ 1000.1 ≤ Pi ≤ 1000. B. Solution这道题还是有点坑，导致第一次交的时候WA了。题目大意是有无限多的食客，其中D个人盘子里有数目不同都煎饼，每分钟每个有煎饼的人吃掉一个煎饼，或担任侍者的人让所有人停止进食，将某个盘子中的若干煎饼移到另一个盘子。求解吃完所有煎饼的最少分钟数。 一个显然的结论是移动煎饼的目的是为了将煎饼分散使得更多人同时吃，因此最开始若干分钟用来移动煎饼，移动完毕后所有人开吃并不再进行移动。这样可以将煎饼尽量分散后开吃。 另一个显然的结论是任意局面要移动煎饼时，总是从煎饼最多的盘子移动到某个空盘子。这些显然的结论不是得到最优解的必要条件，但是使用这些结论考虑问题可以在不影响解的最优性的情况下简化问题的处理过程。 一个错误的想法是这样：每次尝试将最多煎饼的盘子等分成两份。这个错误的想法让我们贡献了一次WA，后来华哥指出了这个错误；具体样例就不给出了。 一种正确的做法是枚举等待时间，算出相应的用于移动的时间相加得出答案。对于一个确定的等待时间分钟数i，所有大于i个煎饼的盘子需要被拆分成数个不超过i个煎饼的盘子。有j个煎饼的盘子需要分ceiling(j/i)-1次。总的时间按复杂度是O(P^2)。 Tips：对于整数，计算ceiling(j/i)=(j+i-1)/i。 C. DijkstraThe Dutch computer scientist Edsger Dijkstra made many important contributions to the field, including the shortest path finding algorithm that bears his name. This problem is not about that algorithm. You were marked down one point on an algorithms exam for misspelling “Dijkstra” – between D and stra, you wrote some number of characters, each of which was either i, j, or k. You are prepared to argue to get your point back using quaternions, an actual number system (extended from complex numbers) with the following multiplicative structure: To multiply one quaternion by another, look at the row for the first quaternion and the column for the second quaternion. For example, to multiply i by j, look in the row for i and the column for j to find that the answer is k. To multiply j by i, look in the row for j and the column for i to find that the answer is -k. As you can see from the above examples, the quaternions are not commutative – that is, there are some a and b for which a b != b a. However they are associative – for any a, b, and c, it’s true that a (b c) = (a b) c. Negative signs before quaternions work as they normally do – for any quaternions a and b, it’s true that -a -b = a b, and -a b = a -b = -(a * b). You want to argue that your misspelling was equivalent to the correct spelling ijk by showing that you can split your string of is, js, and ks in two places, forming three substrings, such that the leftmost substring reduces (under quaternion multiplication) to i, the middle substring reduces to j, and the right substring reduces to k. (For example, jij would be interpreted as j i j; j i is -k, and -k j is i, so jij reduces to i.) If this is possible, you will get your point back. Can you find a way to do it? Input The first line of the input gives the number of test cases, T. T test cases follow. Each consists of one line with two space-separated integers L and X, followed by another line with L characters, all of which are i, j, or k. Note that the string never contains negative signs, 1s, or any other characters. The string that you are to evaluate is the given string of L characters repeated X times. For instance, for L = 4, X = 3, and the given string kiij, your input string would be kiijkiijkiij. Output For each test case, output one line containing “Case #x: y”, where x is the test case number (starting from 1) and y is either YES or NO, depending on whether the string can be broken into three parts that reduce to i, j, and k, in that order, as described above. Limits 1 ≤ T ≤ 100.1 ≤ L ≤ 10000. Small dataset 1 ≤ X ≤ 10000.1 ≤ L * X ≤ 10000. Large dataset 1 ≤ X ≤ 1012.1 ≤ L * X ≤ 1016. C. Solution题目大意是给定一个长度为L*X，由X个长度为L的，由ijk三个因子组成的序列重复而成。ijk三个因子类似于虚数单位i，题目中给出了1ijk相乘的结果。这四个因子的乘法封闭并满足结合律，要求将给定序列拆分成三段，连乘的结果分别是ijk。 可以看出，ijk三个因子的平方都等于-1。因此，每4个长度为L的序列连乘的积等于1。对于一个序列结果为x，那么在这个序列后面加上长度4L倍数的序列，结果不变，我们因此没有必要在大于4L的序列中寻找连乘积为i或j或k的子序列。综上，对于X很大的大数据（X&gt;12），令X’≡X(mod 4)并且X’&gt;12的情况下不影响结果；我们可以将大于12的X缩小到12到15的范围。剩下的工作，我们只需要在长度不超过15*L的序列中暴力验证能否累乘出i和j以及剩下的部分累乘是否等于k即可。 D. Ominous OminoAn N-omino is a two-dimensional shape formed by joining N unit cells fully along their edges in some way. More formally, a 1-omino is a 1x1 unit square, and an N-omino is an (N-1)omino with one or more of its edges joined to an adjacent 1x1 unit square. For the purpose of this problem, we consider two N-ominoes to be the same if one can be transformed into the other via reflection and/or rotation. For example, these are the five possible 4-ominoes: And here are some of the 108 possible 7-ominoes: Richard and Gabriel are going to play a game with the following rules, for some predetermined values of X, R, and C: Richard will choose any one of the possible X-ominoes. Gabriel must use at least one copy of that X-omino, along with arbitrarily many copies of any X-ominoes (which can include the one Richard chose), to completely fill in an R-by-C grid, with no overlaps and no spillover. That is, every cell must be covered by exactly one of the X cells making up an X-omino, and no X-omino can extend outside the grid. Gabriel is allowed to rotate or reflect as many of the X-ominoes as he wants, including the one Richard chose. If Gabriel can completely fill in the grid, he wins; otherwise, Richard wins. Given particular values X, R, and C, can Richard choose an X-omino that will ensure that he wins, or is Gabriel guaranteed to win no matter what Richard chooses? Input The first line of the input gives the number of test cases, T. T lines follow. Each contains three space-separated integers: X, R, and C. Output For each test case, output one line containing “Case #x: y”, where x is the test case number (starting from 1) and y is either RICHARD (if there is at least one choice that ensures victory for Richard) or GABRIEL (if Gabriel will win no matter what Richard chooses). Limits Small dataset T = 64.1 ≤ X, R, C ≤ 4. Large dataset 1 ≤ T ≤ 100.1 ≤ X, R, C ≤ 20. D. Solution题目大意是在R*C的棋盘上放置X-onimo。其中先手Richard给后手Gabriel一个特定的X-omino，Gabriel必须使用至少一次这款X-omino和不限形状和数量的其他X-omino，将棋盘不重不漏不多地覆盖。我们要判断给定X,R,C的情况下，Richard是否必胜。 这道题我并没有想出什么好的算法，但是大数据只有20，因此可以分情况判断先手是否必胜。有如下几种规则： 如果X不是R*C的约数，显然先手胜利。 如果X大于R和C的较大者，可以构造一条棍子，先手必胜。 当X大于等于7的时候，可以形成一个环，环里面显然无法覆盖到，先手胜。 如果X大于R和C中较小者的二倍，可以构造L型的X-omino使得无法放入棋盘。 只要X能整除R*C，且X&lt;3，那么一定是Gabriel胜。 经过以上等几个规则后默认Gabriel胜，交上依然WA。我把以上规则无法覆盖到的情况输出出来，人工看了下这些情况进行了补全。补全之后的检验函数是这个样子的： 1234567891011121314151617181920212223242526bool check(int r, int c, int x) &#123; if (r &lt; c) &#123; int t = r; r = c; c = t; &#125; if (r * c % x != 0) return true; if (x &gt; r) return true; if (x &gt;= 2 * c + 1) return true; if (x &gt;= 7) return true; if (x == 1) return false; if (x == 6 &amp;&amp; c == 3) return true; if (c &gt;= 3 &amp;&amp; x &lt;= 3) return false; if (x &lt;= 2) return false; if (c &gt;= x &amp;&amp; x &lt; 7) return false; if (c == 2 &amp;&amp; x == 4) return true; return false;&#125; 漏掉的情况主要是在宽度2的棋盘上放置X=4的X-omino。然后就过了小数据。 后记 最终第四题的大数据WA掉了，有好的解题方法的可以回复我哦。","categories":[{"name":"Algorithm Contest","slug":"Algorithm-Contest","permalink":"http://tedxz.github.io/categories/Algorithm-Contest/"}],"tags":[{"name":"Algorithm Contest","slug":"Algorithm-Contest","permalink":"http://tedxz.github.io/tags/Algorithm-Contest/"},{"name":"Google Codejam","slug":"Google-Codejam","permalink":"http://tedxz.github.io/tags/Google-Codejam/"}]},{"title":"千岛湖之旅","slug":"trip-to-qian-dao-hu","date":"2015-04-07T16:31:20.000Z","updated":"2016-07-26T16:22:57.366Z","comments":true,"path":"2015/04/08/trip-to-qian-dao-hu/","link":"","permalink":"http://tedxz.github.io/2015/04/08/trip-to-qian-dao-hu/","excerpt":"清明节，忙里偷闲，逃千岛湖。挑几张图。","text":"清明节，忙里偷闲，逃千岛湖。挑几张图。 千岛湖小镇是个漂亮的小镇。 这里的人都很善良。 水很漂亮。 夜景也很漂亮。 岛上风景很好。 去看看这世界。 多说两句首先要感谢楼主借我的相机。相机这个东西其实很不方便借给别人（特别是小白）的，这里我只能说：楼主好人。 行程安排略有失误，但也不负旅途。 回程路上，雨一直下，航班取消。结识了建大的同学和烧烤店的老板，好巧。 幸运的改签了少有的几个位置赶回西安，刻薄的出租司机，和善的拼车大叔；最终在涛哥家过夜。 奇妙的旅程。","categories":[{"name":"Diary","slug":"Diary","permalink":"http://tedxz.github.io/categories/Diary/"}],"tags":[{"name":"Diary","slug":"Diary","permalink":"http://tedxz.github.io/tags/Diary/"},{"name":"Trip","slug":"Trip","permalink":"http://tedxz.github.io/tags/Trip/"}]},{"title":"百度实习生电话面试","slug":"baidu-intern-telephone-interview","date":"2015-03-17T12:26:47.000Z","updated":"2015-03-17T13:20:45.599Z","comments":true,"path":"2015/03/17/baidu-intern-telephone-interview/","link":"","permalink":"http://tedxz.github.io/2015/03/17/baidu-intern-telephone-interview/","excerpt":"转眼已经大三下学期，同学们都各自忙起来，有的准备考研有的准备投简历找实习。俗话说“大四不考研，天天像过年”，我还是决定不进入考研的深坑，毕竟有保研本校保底。 昨天和同学一行4人跑了好远去参加CVTE的面试，整个面试流程非常水，只有HR问简单的非技术问题,感觉不开心。下午回来收到了百度HR妹子打来的电话，说简历通过，安排了今天上午的电话面试。晚上接到CVTE邀请二面的电话，于是就说明了不能去参加。","text":"转眼已经大三下学期，同学们都各自忙起来，有的准备考研有的准备投简历找实习。俗话说“大四不考研，天天像过年”，我还是决定不进入考研的深坑，毕竟有保研本校保底。 昨天和同学一行4人跑了好远去参加CVTE的面试，整个面试流程非常水，只有HR问简单的非技术问题,感觉不开心。下午回来收到了百度HR妹子打来的电话，说简历通过，安排了今天上午的电话面试。晚上接到CVTE邀请二面的电话，于是就说明了不能去参加。 上午的电面总的来说表现的还是不错。因为邮件上提到，以为要用collabedit这种神奇的工具在线码代码，最终还是没有。整个面试环节都比较顺利，面试官哥哥的声音也蛮让人觉得亲近。面试的问题涉及到C/C++语言基础、Linux的简单知识（非常囧的没有答上来关于core文件的问题），基础的算法知识以及算法应用的问题（自然语言处理领域和搜索领域的简单问题）。大公司面试的时候面试官都不会刻意为难面试者，不会的问题不会缠着不放，比较难的问题也会一步步引导着你回答，因此是不错的面试经历。 由于去年面试阿里实习生的时候在终面交叉面试以“是大二的学生”而被挂掉（过了两轮电面），多少也会对自己的水平缺少一点信心（万一年级过低只是借口呢），因而面试之前还有些担心。不过今天的面试还是受到了面试官的好评和“欢迎来实习”的邀请，预计应该能够顺利通过。 能去熊厂实习还是很开心的，上熊掌一张。由于不清楚公开面试题目是不是合适，这里就不贴具体的面试题咯，有想了解的可以单独联系我。","categories":[{"name":"Diary","slug":"Diary","permalink":"http://tedxz.github.io/categories/Diary/"}],"tags":[{"name":"Diary","slug":"Diary","permalink":"http://tedxz.github.io/tags/Diary/"},{"name":"Intern","slug":"Intern","permalink":"http://tedxz.github.io/tags/Intern/"}]},{"title":"Machine Learning Note (6) - Regularization","slug":"ml-note-6-regularization","date":"2015-03-09T13:06:49.000Z","updated":"2016-07-27T09:54:17.930Z","comments":true,"path":"2015/03/09/ml-note-6-regularization/","link":"","permalink":"http://tedxz.github.io/2015/03/09/ml-note-6-regularization/","excerpt":"In linear regression model and logistic regression model, overfitting is a common problem. Regularization is a way to help us solving this problem. Today I will talk about overfitting and regularization.","text":"In linear regression model and logistic regression model, overfitting is a common problem. Regularization is a way to help us solving this problem. Today I will talk about overfitting and regularization. OverfittingIf we have too many features, the learned hypothesis may fit the training set very well, but fail to generalize new examples. Overfitting often happen when we use a powerful hypothesis, such as using too many features or using high order polynomial terms, etc. To avoid overfitting, we need to prevent our hypothesis having too much power. RegularizationRegularization is a way to limit the power of hypothesis while reducing features is another one. The idea of regularization is to penalize parameter θ{\\theta}θ being big values to make our hypothesis &quot;simple&quot;. To do so, we are going to add the cost of using features to our cost function: λ2m∑j=1nθj2.\\frac{\\lambda }{{2m}}\\sum\\limits_{j = 1}^n {\\theta_j^2} .​2m​​λ​​​j=1​∑​n​​θ​j​2​​. For linear regression, the regularized cost function is: J(θ)=12m∑i=1m(h(x(i))−y(i))2+λ2m∑j=1nθj2.J(\\theta ) = \\frac{1}{{2m}}\\sum\\limits_{i = 1}^m {{{(h({x^{(i)}}) - {y^{(i)}})}^2}} + \\frac{\\lambda }{{2m}}\\sum\\limits_{j = 1}^n {\\theta_j^2} .J(θ)=​2m​​1​​​i=1​∑​m​​(h(x​(i)​​)−y​(i)​​)​2​​+​2m​​λ​​​j=1​∑​n​​θ​j​2​​. And the regularized cost function of logistic regression is: J(θ)=−1m[∑i=1my(i)logh(x(i))+(1−y(i))log(1−h(x(i)))]+λ2m∑j=1nθj2J(\\theta ) = - \\frac{1}{m}[\\sum\\limits_{i = 1}^m {} {y^{(i)}}\\log h({x^{(i)}}) + (1 - {y^{(i)}})\\log (1 - h({x^{(i)}}))] + \\frac{\\lambda }{{2m}}\\sum\\limits_{j = 1}^n {\\theta_j^2}J(θ)=−​m​​1​​[​i=1​∑​m​​y​(i)​​logh(x​(i)​​)+(1−y​(i)​​)log(1−h(x​(i)​​))]+​2m​​λ​​​j=1​∑​n​​θ​j​2​​ A detile that need to be emphasized is that θ0{\\theta_0}θ​0​​ should not be penalized. Regularization ParameterYou may notice that a undefined symbol λ{\\lambda}λ, which is the regularization parameter. It controls a trade off between fitting the training data well and keeping the parameters small to avoid overfitting. When λ{\\lambda}λ is small, the hypothesis is likely to have more power of fitting the training data well; when λ{\\lambda}λ is equal to 0, the hypothesis becomes unregularized. And as λ{\\lambda}λ goes bigger, we take more penalization on bigger parameter, so that we can get a hypothesis relatively simple to avoid overfitting. So how can we choose a properate λ{\\lambda}λ to improve our hypohtesis&apos;s performance on generalized data? A considerable way is to try different λ{\\lambda}λ and choose the one that performs best on the validation data. Validation data set is disjoint from the training data set randomly chosen from known data set. I&apos;m not going to talk much about validation here. According to Andrew Ng&apos;s lecture, it is common that try different λ=0.01,0.03,0.1,0.3,1,3,...\\lambda = 0.01,0.03,0.1,0.3,1,3,...λ=0.01,0.03,0.1,0.3,1,3,... and choose the best one.","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://tedxz.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://tedxz.github.io/tags/Machine-Learning/"},{"name":"Regularization","slug":"Regularization","permalink":"http://tedxz.github.io/tags/Regularization/"}]},{"title":"Machine Learning Note (5) - Logistic Regression","slug":"ml-note-5-logistic-regression","date":"2015-03-06T11:22:44.000Z","updated":"2016-07-27T09:50:27.761Z","comments":true,"path":"2015/03/06/ml-note-5-logistic-regression/","link":"","permalink":"http://tedxz.github.io/2015/03/06/ml-note-5-logistic-regression/","excerpt":"Logistic Regression is a type of the classification model rather than regression model. It is used to predict a binary response based on known features. For some reason, I&apos;ve talked about SVM, which is also a supervised binary classification model, and much complicated than logistic regression, so I can omit some basic specification in this article.","text":"Logistic Regression is a type of the classification model rather than regression model. It is used to predict a binary response based on known features. For some reason, I&apos;ve talked about SVM, which is also a supervised binary classification model, and much complicated than logistic regression, so I can omit some basic specification in this article. Lead UpLinear regression is a model that can find out the relationship between input and output, but it&apos;s not good at dealing with some problem like this:. In this problem, the value of y is either 0 or 1. It is clear that there is a boundary between malignant tumor and benign ones according to their size. With linear regression and a threshold value equal to 0.5, we may solve this problem by comparing the output value with the threshold, and giving a prediction of 0 if output is less than threshold, and 1 otherwise. But there are still some problem: if there is a malignant tumor of a big size, it will observably decrease the slope of the straight line of linear regression hypothesis, but we can notice that such an example should not influence out hypothesis intuitively. In general, solve the classification problem with linear regression is not a good idea. Logistic regression is a proper model to solve this problem. Logistic Regression HypothesisLogistic regression model can be represented as this: hθ(x)=g(θTx).{h_\\theta }(x) = g({\\theta ^T}x).h​θ​​(x)=g(θ​T​​x). Here g(x)g(x)g(x) is logistic function or sigmoid function:g(z)=11+e−zg(z) = \\frac{1}{{1 + {e^{ - z}}}}g(z)=​1+e​−z​​​​1​​ which can map a real number to [0, 1]. The hypothesis represents the estimated probability that y=1y=1y=1 on input xxx. Since the hypothesis gives us a probability of y=1y=1y=1, we can predict y=1y=1y=1 if hθ(x)⩾0.5{h_\\theta }(x) \\geqslant 0.5h​θ​​(x)⩾0.5 and y=0y=0y=0 otherwise. We can also adjust the threshold value higher or lower to improve the prediction. Polynomial FeaturesThe decision boundary of logistic regression is a hyperplane in a hyperspace. But for some complicated problems, the examples can not be separated by a hyperplane. Here is a trivial example:. A common way to enhance our model is to use high order polynomial terms to the features. Added squared features, the hypothesis will be like this: hθ(x)=g(θ0+θ1x1+θ2x2+θ3x12+θ4x22){h_\\theta }(x) = g({\\theta_0} + {\\theta_1}{x_1} + {\\theta_2}{x_2} + {\\theta_3}x_1^2 + {\\theta_4}x_2^2)h​θ​​(x)=g(θ​0​​+θ​1​​x​1​​+θ​2​​x​2​​+θ​3​​x​1​2​​+θ​4​​x​2​2​​) This is also useful in linear regression and many other models. Added more higher order polynomial feature lead to a more powerful hypothesis, but also more computation cost and risk of over fitting. ##Cost FuntionSo how can we choose parameters θ\\thetaθ? We define cost function J(θ)J(\\theta )J(θ) to evaluate θ\\thetaθ. It&apos;s kind of different from the cost function of linear regression. The cost function is defined as follow : J(θ)=−1m[∑i=1my(i)loghθ(x(i))+(1−y(i))log(1−hθ(x(i)))].J(\\theta ) = - \\frac{1}{m}[\\sum_{i = 1}^m {} {y^{(i)}}log {h_\\theta }({x^{(i)}}) + (1 - {y^{(i)}})log (1 - {h_\\theta }({x^{(i)}}))].J(θ)=−​m​​1​​[​i=1​∑​m​​y​(i)​​logh​θ​​(x​(i)​​)+(1−y​(i)​​)log(1−h​θ​​(x​(i)​​))]. The superscript (i) means the ith example while m is the number of examples. To find the θ\\thetaθ that minimizes J(θ)J(\\theta )J(θ), gradient descent algorithm is needed. RegularizationTo avoid overfitting when there are too many features or applying high order polynomial features, we should punish our hypothesis being too &quot;powerful&quot;. The technique is call regularization, which is going to be introduced in the next note. Feature Scaling and Mean NormalizationFeature scaling is to get every feature into approximately a −1⩽xi⩽1- 1 \\leqslant {x_i} \\leqslant 1−1⩽x​i​​⩽1 range.Mean normalization is to replace xi{x_i}x​i​​ with xi−μi{x_i} - {\\mu_i}x​i​​−μ​i​​ to make features have approximately 0 mean(Do not apply to x0=1{x_0} = 1x​0​​=1).Making sure features are on a similar scale usually can improve the efficiency of gradient descent.","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://tedxz.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://tedxz.github.io/tags/Machine-Learning/"},{"name":"Logistic Regression","slug":"Logistic-Regression","permalink":"http://tedxz.github.io/tags/Logistic-Regression/"}]},{"title":"My Second Coursera Certification","slug":"my-second-coursera-certification","date":"2015-02-08T10:11:32.000Z","updated":"2015-02-08T11:43:43.514Z","comments":true,"path":"2015/02/08/my-second-coursera-certification/","link":"","permalink":"http://tedxz.github.io/2015/02/08/my-second-coursera-certification/","excerpt":"So long after the terrible result of “Introduction to Artificial Intelligence“ last semester comes out, the statement of accomplishment of course “Artificial Intelligence“ by NTU comes as well. It is a great consolation to my heart broken by the poor score after my hard work. An interesting thing is that I got more than 100 points (101 out of 100) in the course.","text":"So long after the terrible result of “Introduction to Artificial Intelligence“ last semester comes out, the statement of accomplishment of course “Artificial Intelligence“ by NTU comes as well. It is a great consolation to my heart broken by the poor score after my hard work. An interesting thing is that I got more than 100 points (101 out of 100) in the course.Here is the certification. Another certification is for the coursera “Machine Learning Fondations“, which is the first certification I get from MOOC.","categories":[{"name":"Diary","slug":"Diary","permalink":"http://tedxz.github.io/categories/Diary/"}],"tags":[{"name":"Diary","slug":"Diary","permalink":"http://tedxz.github.io/tags/Diary/"},{"name":"Coursera","slug":"Coursera","permalink":"http://tedxz.github.io/tags/Coursera/"}]},{"title":"Machine Learning Note (4) - Linear Regression","slug":"ml-note-4-linear-regression","date":"2015-02-02T09:15:35.000Z","updated":"2016-07-27T09:43:50.966Z","comments":true,"path":"2015/02/02/ml-note-4-linear-regression/","link":"","permalink":"http://tedxz.github.io/2015/02/02/ml-note-4-linear-regression/","excerpt":"Both regression and classification are typical supervised learning problem, where there is an input X{X}X, output Y{Y}Y, and both X{X}X and Y{Y}Y is continuous (such as X∈RdX \\in {R^d}X∈R​d​​ and Y∈RY \\in RY∈R). The task is to discover the relationship between X{X}X and Y{Y}Y. According to different kinds of relationships between X{X}X and Y{Y}Y, we use different models to solve them.","text":"Both regression and classification are typical supervised learning problem, where there is an input X{X}X, output Y{Y}Y, and both X{X}X and Y{Y}Y is continuous (such as X∈RdX \\in {R^d}X∈R​d​​ and Y∈RY \\in RY∈R). The task is to discover the relationship between X{X}X and Y{Y}Y. According to different kinds of relationships between X{X}X and Y{Y}Y, we use different models to solve them. Linear RegressionLinear regression is a kind of problem that helps us to find the linear relation between input X{X}X and output Y{Y}Y so that when a new group of input data X′{X^\\prime}X​′​​ comes to us, we can predict the output Y′{Y^\\prime}Y​′​​ related to X′{X^\\prime}X​′​​. Let&apos;s have an example of house price predicting (I use the example in Machine Learning lecture of Andrew Ng on Coursera because I&apos;m lazy). Suppose that a house has some features related to its price, including size (feet2{feet^2}feet​2​​), number of bedrooms, number of floors, and age of home (years). We consider that the price of the house will have a linear relation with the features. So out hypothesis is like this: hw(x)=w0+w1x1+w2x2+w3x3+w4x4.{h_w}(x) = {w_0} + {w_1}{x_1} + {w_2}{x_2} + {w_3}{x_3} + {w_4}{x_4}.h​w​​(x)=w​0​​+w​1​​x​1​​+w​2​​x​2​​+w​3​​x​3​​+w​4​​x​4​​. To keep the notation simple, define x0=1{x_0=1}x​0​​=1 , and we are able to represent the hypothesis as following form: hw(x)=wTx{h_w}(x) = {w^T}xh​w​​(x)=w​T​​x where x{x}x and w{w}w are presented as follow: x=[x0,x1,x2,x3,x4]T∈Rn+1x = {[{x_0},{x_1},{x_2},{x_3},{x_4}]^T} \\in {{R}^{n + 1}}x=[x​0​​,x​1​​,x​2​​,x​3​​,x​4​​]​T​​∈R​n+1​​ w=[w0,w1,w2,w3,w4]T∈Rn+1{w} = {[{{w}_0},{{w}_1},{{w}_2},{{w}_3},{{w}_4}]^T} \\in {{R}^{n + 1}}w=[w​0​​,w​1​​,w​2​​,w​3​​,w​4​​]​T​​∈R​n+1​​ Cost Function / Error FunctionWith different w{w}w, we have different hypotheses hw{h_{w}}h​w​​ to describe the relation between input and output. So how can we choose the best hypothesis among them? We define the cost function J{J}J (or error function E{E}E in different references) to measure the error of the hypotheses. A frequently-used error measure is Mean Squared Error: E(w)=1N∑n=1N(hw(xn)−yn)2{E}(w) = \\frac{1}{N}\\sum_{n = 1}^N {{{({h_w}({x_n}) - {y_n})}^2}}E(w)=​N​​1​​​n=1​∑​N​​(h​w​​(x​n​​)−y​n​​)​2​​ Here N is the number of the examples, and xn{x_n}x​n​​ means the nth example instead of the nth feature of the example. To eliminate the summation, the above formula can be written as: E(w)=1N∣∣Xw−y∣∣2E(w) = \\frac{1}{N}||Xw - y|{|^2}E(w)=​N​​1​​∣∣Xw−y∣∣​2​​ To choose the best hypothesis, we want to minimize error over w{w}w. Our goal is: minwE(w)=1N∣∣Xw−y∣∣2\\min_w E(w) = \\frac{1}{N}||Xw - y||^2​w​min​​E(w)=​N​​1​​∣∣Xw−y∣∣​2​​ Solve Linear Regression ProblemNotice that error function E(w){E(w)}E(w) is continuous, differentiable and convex, we can know that necessary condition for E(w){E(w)}E(w) getting minimum is: ∇E(w)=[∂E∂w0(w),∂E∂w1(w),...,∂E∂wd(w)]T=[0,0,...,0]T\\nabla E(w) = {[\\frac{{\\partial E}}{{\\partial {w_0}}}(w),\\frac{{\\partial E}}{{\\partial {w_1}}}(w),...,\\frac{{\\partial E}}{{\\partial {w_d}}}(w)]^T} = {[0,0,...,0]^T}∇E(w)=[​∂w​0​​​​∂E​​(w),​∂w​1​​​​∂E​​(w),...,​∂w​d​​​​∂E​​(w)]​T​​=[0,0,...,0]​T​​ And we can work out that the gradient of E(w){E(w)}E(w) is: ∇E(w)=2N(XTXw−XTy)\\nabla E(w) = \\frac{2}{N}({X^T}Xw - {X^T}y)∇E(w)=​N​​2​​(X​T​​Xw−X​T​​y) To have ∇E(w)=0\\nabla E(w) = 0∇E(w)=0, we can get: wL=(XTX)−1XTy{w_L} = {({X^T}X)^{ - 1}}{X^T}yw​L​​=(X​T​​X)​−1​​X​T​​y when XTX{X^T}XX​T​​X is invertible. But what if XTX{X^T}XX​T​​X is singular? We can use X†{X^\\dagger }X​†​​, which is known as pseudoinverse of X{X}X, instead of (XTX)−1XT{({X^T}X)^{ - 1}}{X^T}(X​T​​X)​−1​​X​T​​. In practice, we always use wL=X†y{w_L} = {X^ \\dagger }yw​L​​=X​†​​y to calculate wL{w_L}w​L​​, the optimal w{w}w that makes the hypothesis fit the examples best. By the way, we can use function pinv(x) in Matlab / Octave to calculate the pseudoinverse of a matrix, and in Andrew Ng’s ML lecture, he uses pinv(x&#39; * x) * x&#39; instead, where x&#39; * x is a square matrix, but not necessary. Gradient DescentGradient Descent is another way to find a local minimum of a function. I will show it in the future.","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://tedxz.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://tedxz.github.io/tags/Machine-Learning/"},{"name":"Linear Regression","slug":"Linear-Regression","permalink":"http://tedxz.github.io/tags/Linear-Regression/"}]},{"title":"在双系统的电脑上共用蓝牙设备","slug":"pair-bluetooth-on-dualboot-computer","date":"2015-01-29T13:36:08.000Z","updated":"2015-02-03T08:27:28.433Z","comments":true,"path":"2015/01/29/pair-bluetooth-on-dualboot-computer/","link":"","permalink":"http://tedxz.github.io/2015/01/29/pair-bluetooth-on-dualboot-computer/","excerpt":"前些日子终于拿到了期盼已久的Filco Minila，今天切到Debian下面撸代码发现双系统切换的时候需要重新配对非常麻烦，于是着手解决这个问题。 蓝牙设备连接时有一个128位的Link Key用于对数据进行加密，两个系统共用一个蓝牙适配器但是却具有不同的Link Key故切换系统后不能直接连接。因此解决这个问题的方法是将两个系统分别配对后同一个蓝牙设备的Link Key修改为相同的值，切换系统之后就能自动连接了。","text":"前些日子终于拿到了期盼已久的Filco Minila，今天切到Debian下面撸代码发现双系统切换的时候需要重新配对非常麻烦，于是着手解决这个问题。 蓝牙设备连接时有一个128位的Link Key用于对数据进行加密，两个系统共用一个蓝牙适配器但是却具有不同的Link Key故切换系统后不能直接连接。因此解决这个问题的方法是将两个系统分别配对后同一个蓝牙设备的Link Key修改为相同的值，切换系统之后就能自动连接了。 下面说一下查看和修改Link Key的方法： Windows： Windows的Link Key存在注册表里，但是由于相应的注册表项即使以管理员身份也无法查看，因此需要借助工具psexec（在微软这儿下载）。 把下到的工具解压后得到psexec.exe等工具，该路径下命令行运行： psexec -s -i regedit 以系统权限运行注册表编辑器。 在注册表项SYSTEM\\ControlSet002\\services\\BTHPORT\\Parameters\\Keys\\[BD_ADDR]中查看名称为蓝牙设备MAC地址的注册表键对应的值，就是Link Key。可以修改或记下来待会去另一个系统下改。 Linux： Link Key信息存在/var/lib/Bluetooth/[BD_ADDR]/这个路径下，其中[BD_ADDR]是电脑蓝牙适配器的MAC地址，一般只有一个，除非你有多块蓝牙适配器。 root权限修改linkkeys这个文件，里面一行对应了链接的一个外设。根据你要修改的外设MAC地址找到对应的行，查看修改中间一长串16进制数值。格式一看就懂了不多说。修改完重启。 好了说到这儿你们也该知道我写这个是为了秀键盘的，上图：","categories":[{"name":"Skill","slug":"Skill","permalink":"http://tedxz.github.io/categories/Skill/"}],"tags":[{"name":"Skill","slug":"Skill","permalink":"http://tedxz.github.io/tags/Skill/"},{"name":"Dualboot","slug":"Dualboot","permalink":"http://tedxz.github.io/tags/Dualboot/"},{"name":"Bluetooth","slug":"Bluetooth","permalink":"http://tedxz.github.io/tags/Bluetooth/"}]},{"title":"庆祝Blog迁移至Github Pages","slug":"celebrate-moving-blog","date":"2015-01-27T18:26:14.000Z","updated":"2015-04-18T15:45:58.352Z","comments":true,"path":"2015/01/28/celebrate-moving-blog/","link":"","permalink":"http://tedxz.github.io/2015/01/28/celebrate-moving-blog/","excerpt":"今天终于把博客搬到了Github pages上面，用Hexo搞定了页面生成。终于不用再受CSDN淡腾的审核（有点英文就要审核好久，晚上写的还要等到第二天）和坑爹的文本编辑器了。","text":"今天终于把博客搬到了Github pages上面，用Hexo搞定了页面生成。终于不用再受CSDN淡腾的审核（有点英文就要审核好久，晚上写的还要等到第二天）和坑爹的文本编辑器了。转移Blog的时候偶然发现了MathJax这个神器(keng)，于是就把前几日写的ML note里的公式全都转成了MathJax。。对此我只能表示强迫症患者伤不起。没搬来的Blog明天继续。 搞到这个点明天估计起床不能了，先给实验室请个假，挖个坑改天记一下Hexo和MathJax的使用。 Good night.","categories":[{"name":"Diary","slug":"Diary","permalink":"http://tedxz.github.io/categories/Diary/"}],"tags":[{"name":"Diary","slug":"Diary","permalink":"http://tedxz.github.io/tags/Diary/"}]},{"title":"Machine Learning Note (3) - Support Vector Machine","slug":"ml-note-3-support-vector-machine","date":"2015-01-21T03:56:07.000Z","updated":"2016-07-27T08:20:52.993Z","comments":true,"path":"2015/01/21/ml-note-3-support-vector-machine/","link":"","permalink":"http://tedxz.github.io/2015/01/21/ml-note-3-support-vector-machine/","excerpt":"(Some figures and formulas in this article come from references) In general, SVM is ought to be written after some basic algorithms like perceptron learning algorithm, logistic regression, etc. But due to limited time, and upcoming reporting of tomorrow&apos;s regular meeting, I have to summarize the knowledge that have learned today.","text":"(Some figures and formulas in this article come from references) In general, SVM is ought to be written after some basic algorithms like perceptron learning algorithm, logistic regression, etc. But due to limited time, and upcoming reporting of tomorrow&apos;s regular meeting, I have to summarize the knowledge that have learned today. What is Support Vector MachineSupport Vector Machine is a binary classification model. It constructs a maximum margin separator in the feature domain. SVMs create a linear separating hyperplane, but thanks to kernel trick, SVMs are able to embed the data into a higher-dimensional space efficiently so that SVMs have ability to handle with linear non-separable data. SVMs can also deal with the data with noise, as soft-margin SVMs allow data points to violate the margin, or even be misclassified. Hard-margin SVMRecall our knowledge of linear classification problem, a linear classifier is a hyperplane in the feature domain. An example of four points in a 2-dimentional with a possible classifier is as follow:PLA algorithm cannot figure out the difference of the three classifier, but out intuition tells us that the third one is better – it distance the points further. Without explain in detail, we consider that a classifier with further distance to the point has a bigger tolerance range than the one with no distance. The following figure shows that clearly:This is the reason why we want to find a linear separator with a large margin. Here margin means the distance from the hyperplane to the nearest points:It is obvious that the third classifier has the largest margin. A hyperplane with the larger margin has following advantages: more noise tolerance; generalize better with lower VC dimension; out-of-sample error can be small even in large dimension. Due to above reasons, we are willing to find the SVM – the linear classifier with largest margin. If you want to know more about proof of the advantages, please refer to reference [1]. Skipping over the explanation, we describe the problem of finding the separator with the largest margin as a following optimization problem. minimizeb,w:12wTw{\\mathrm{minimize}}_{b,w} :{1 \\over 2}{w^T}wminimize​b,w​​:​2​​1​​w​T​​w subject to:minn=1,...,Nyn(wTxn+b)=1.{\\mathrm{subject\\ to}}:\\min_{n = 1,...,N} {y_n}({w^T}{x_n} + b) = 1.subject to:​n=1,...,N​min​​y​n​​(w​T​​x​n​​+b)=1. The above problem is equal to the following one as long as there are both positive and negative examples in the data, and the following one is much easier to solve: minimizeb,w:12wTw{{\\mathrm{minimize}}}_{b,w} :{1 \\over 2}{w^T}wminimize​b,w​​:​2​​1​​w​T​​w subject to:yn(wTxn+b)≥1.{\\mathrm{subject\\ to}}:{y_n}({w^T}{x_n} + b) \\ge 1.subject to:y​n​​(w​T​​x​n​​+b)≥1. This optimization problem belongs to Quadratic Programming, which is well studied. A standard QP problem is described as following form: minimizeu∈RL:12uTQu+pTu{{\\text{minimize}}}_{u \\in {R^L}} :\\frac{1}{2}{u^T}Qu + {p^T}uminimize​u∈R​L​​​​:​2​​1​​u​T​​Qu+p​T​​u subjectto:amTu⩾cm(m=1,...,M).{\\text{subject}}\\;{\\text{to}}:a_m^Tu \\geqslant {c_m}(m = 1,...,M).subjectto:a​m​T​​u⩾c​m​​(m=1,...,M). The hard-margin SVM can be solved with QP: Dual formulation of the SVM(Dual formulation of SVM involves heavy math, I will not describe this part in detail. If you want to know more, please check the reference.) The original problem also can be presented with Lagrangian function: L(u,a)=12uTQu+pTu+α(c−aTu)L(u,a) = \\tfrac{1}{2}{u^T}Qu + {p^T}u + \\alpha (c - {a^T}u)L(u,a)=​2​​1​​u​T​​Qu+p​T​​u+α(c−a​T​​u) and the optimization problem is:minumaxa≥0L(u,a).\\min_u \\max_{a \\ge 0} L(u,a).​u​min​​​a≥0​max​​L(u,a). It is not so easy to solve the above problem, and the dual problem comes to us. minumaxa≥0L(u,a)=maxa≥0minuL(u,a)\\min_u \\max_{a \\ge 0} L(u,a) = \\max_{a \\ge 0} \\min_u L(u,a)​u​min​​​a≥0​max​​L(u,a)=​a≥0​max​​​u​min​​L(u,a) The above formula is not always true. It is true if KKT conditions are satisfied. The purpose of we involving dual problem is to simplify solving of the prime problem, furthermore, we can extend SVM to solve linear non-separable data together with dual view of SVM. Kernel Trick for SVMRecall our experience of handling linear non-separable data, we mapped data in the X-space into Z-space. In dual SVM problem, classifier function is as follow: f(x)=(∑i=1Naiyixi)Tx+bf(x) = (\\sum_{i = 1}^N {{a_i}{y_i}{x_i}{)^T}x + b}f(x)=(​i=1​∑​N​​a​i​​y​i​​x​i​​)​T​​x+b f(x)=∑i=1Naiyi&lt;xi,x&gt;+bf(x) = \\sum_{i = 1}^N {{a_i}{y_i} &lt; {x_i},x &gt; + b}f(x)=​i=1​∑​N​​a​i​​y​i​​&lt;x​i​​,x&gt;+b and transform function is:z=ϕ(x)z = \\phi(x)z=ϕ(x)by combining the two formula, we can get:f(x)=∑i=1laiyi&lt;ϕ(xi),ϕ(x)&gt;+b.f(x) = \\sum_{i = 1}^l {{a_i}{y_i} &lt; \\phi ({x_i}),\\phi (x) &gt; + b}.f(x)=​i=1​∑​l​​a​i​​y​i​​&lt;ϕ(x​i​​),ϕ(x)&gt;+b. But as we all know, if X-space has many dimensions, or Z-space is a combination of higher order term, the number of dimension of Z-space will growing explosively. Kernel function is a tool to simplify calculation so that computers are able to calcite classify function quickly. Notice that in order to calculate the classifier function, we need to know the inner product of two vectors after transform, without the two vectors themselves. With kernel function, we can calculate &lt;ϕ(xi),ϕ(x)&gt;&lt; \\phi ({x_i}),\\phi (x) &gt;&lt;ϕ(x​i​​),ϕ(x)&gt; with xi and x directly and quickly, instead of calculate ϕ(xi)\\phi ({x_i})ϕ(x​i​​) and ϕ(x)\\phi ({x})ϕ(x) first. This makes it feasible to solve the problem even if in an infinite dimension space. Based on the above statement, we can infer that the kernel function is a kind of function that calculates inner production of vectors which is mapped into Z-space. For example, let the kernel function K is:K(x1,x2)=(&lt;x1,x2&gt;+1)2K({x_1},{x_2}) = {( &lt; {x_1},{x_2} &gt; + 1)^2}K(x​1​​,x​2​​)=(&lt;x​1​​,x​2​​&gt;+1)​2​​and(&lt;x1,x2&gt;+1)2=2η1ξ1+η12ξ12+2η2ξ2+η22ξ22+2η1η2ξ1ξ2+1.{( &lt; {x_1},{x_2} &gt; + 1)^2} = 2{\\eta_1}{\\xi_1} + \\eta_1^2\\xi_1^2 + 2{\\eta_2}{\\xi_2} + \\eta_2^2\\xi_2^2 + 2{\\eta_1}{\\eta_2}{\\xi_1}{\\xi_2} + 1.(&lt;x​1​​,x​2​​&gt;+1)​2​​=2η​1​​ξ​1​​+η​1​2​​ξ​1​2​​+2η​2​​ξ​2​​+η​2​2​​ξ​2​2​​+2η​1​​η​2​​ξ​1​​ξ​2​​+1. Substituting K(x1,x2)K({x_1},{x_2})K(x​1​​,x​2​​) for &lt;x1,x2&gt;&lt;{x_1},{x_2}&gt;&lt;x​1​​,x​2​​&gt;, is the same as substituting ϕ(x1)\\phi({x_1})ϕ(x​1​​) for x1{x_1}x​1​​ and ϕ(x2)\\phi({x_2})ϕ(x​2​​) for x2{x_2}x​2​​ where ϕ\\phiϕ maps (x1,x2)({x_1},{x_2})(x​1​​,x​2​​) into a higher-dimension space:ϕ(x1,x2)=(2x1,x12,2x2,x22,2x1x2,1)T.\\phi ({x_1},{x_2}) = {(\\sqrt 2 {x_1},x_1^2,\\sqrt 2 {x_2},x_2^2,\\sqrt 2 {x_1}{x_2},1)^T}.ϕ(x​1​​,x​2​​)=(√​2​​​x​1​​,x​1​2​​,√​2​​​x​2​​,x​2​2​​,√​2​​​x​1​​x​2​​,1)​T​​. There are some different kernel function that can map the feature domain into different Z-space. Such as:Polynomial kernel: K(x1,x2)=(&lt;x1,x2&gt;+R)dK({x_1},{x_2}) = {(&lt;{x_1},{x_2}&gt;+R)^d}K(x​1​​,x​2​​)=(&lt;x​1​​,x​2​​&gt;+R)​d​​Gaussian Kernel: K(x1,x2)=exp(−∣∣x1−x2∣∣22σ2)K({x_1},{x_2}) = {\\exp(\\frac{-||{x_1}-{x_2}||^2}{2\\sigma^2})}K(x​1​​,x​2​​)=exp(​2σ​2​​​​−∣∣x​1​​−x​2​​∣∣​2​​​​).Different kernel functions are suitable for different problems. Reference[1] July, 支持向量机通俗导论（理解SVM的三层境界）[2] Abu-Mostafa, Magdon-Ismail, Lin, Learn from data e-Chap 8: Support Vector Machine","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://tedxz.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://tedxz.github.io/tags/Machine-Learning/"},{"name":"Supprot Vector Machine","slug":"Supprot-Vector-Machine","permalink":"http://tedxz.github.io/tags/Supprot-Vector-Machine/"}]},{"title":"Machine Learning Note (2) - Decision Tree","slug":"ml-note-2-decision-tree","date":"2015-01-01T22:25:26.000Z","updated":"2016-07-26T17:03:11.027Z","comments":true,"path":"2015/01/02/ml-note-2-decision-tree/","link":"","permalink":"http://tedxz.github.io/2015/01/02/ml-note-2-decision-tree/","excerpt":"Decision tree is one of the simplest and yet most successful forms of machine learning, so I decided to begin with it. It seems that I am not able to finish this part in a short time, because it isn’t a simple job to reach every aspect of it. I’m going to write the summary of the models and algorithms to help understanding, without a detailed description. Some implementation defiles may be added in the future.","text":"Decision tree is one of the simplest and yet most successful forms of machine learning, so I decided to begin with it. It seems that I am not able to finish this part in a short time, because it isn’t a simple job to reach every aspect of it. I’m going to write the summary of the models and algorithms to help understanding, without a detailed description. Some implementation defiles may be added in the future. What is a decision treeThe input of a decision tree often presented as a vector of attribute values. A decision tree performs a sequence of tests to the input vector. Every inner node represents a test on an particular attribute of input vector and separate input samples into different types, which corresponding to the child nodes of it. A leaf node specified a value to be returned. The return value may be a decision (discrete / boolean), or a score (continuous), etc. When the decision tree takes an input vector, the input vector applies the tests from the root node until reaching a leaf node getting a return value. Tests on each inner node tell which successor node to go. When reaching a leaf node, a decision is made as the node indicates. The nodes that the input vector goes through consists a path from the root to a leaf. Let’s take an example that the input vector has n boolean attribute and output is also a boolean value. In this situation, the decision tree is equivalent to a boolean function. Recall our memory of Karnough map, each node on the tree split a square box on the K-map into two. Decision tree learningNow we have a collection of known datum – input vectors (different items, situations, etc) with their output (decisions, categories, etc.). How to induce a decision tree with the examples, and further, induce a minimum decision tree? In fact, it is an intractable problem to find the minimum tree consistent with the examples, but we are able to find a small (not smallest) consistent tree. To find a small consistent tree, our decision-tree-learning algorithm choice the most “important” attribute to test each time so that we can divide the problem into smaller sub problems. And in this way, we expect that we can get a small tree. Inducing decision treeWe consider the attribute X is an “important” attribute if the examples with different value of attribute X have different output, and the examples with the same value of attribute X have the same output, so that if we test the examples with attribute X, we split examples up with different output. After an attribute test, each outcome is a new decision tree problem with fewer examples and one less attribute. There are four cases to consider for these recursive problems: If the remaining examples are with the same output, then we are done. We can answer the question refer to the remaining examples. If there are no examples left, it means that no example has been observed for this combination of attribute values. We can just give a default answer or a value calculated from the plurality classification of the examples in the parent of constructing node. If there are still some examples with different output, then choose the best attribute to split them. If there is no attribute left, but examples with different output values, it means that these examples have exactly the same description, but different classifications. This can happen because there is an error or noise in the data; because the domain is nondeterministic; or because we didn’t observe an attribute that would distinguish the examples. We can just return the plurality classification of the remaining examples. Choose attribute testsA problem comes to us that how can we measure how important an attribute is. We will use the notion information gain, which is defined in terms of entropy. Entropy is a measure of the uncertainty of a random variable. Here are some examples of variables with different entropy: A random variable with only one value has no uncertainty thus its entropy is defined as zero, such as a coin that always comes up heads. A fair coin with equally likely to come up heads and tails, 0 or 1, counted as “1 bit” of entropy. A fair eight-sided die has 3 bits of entropy. An unfair coin comes up heads 99% of the time has nearly 0 of entropy but positive. In general, the entropy of a random variable VVV with values vkv_kv​k​​, each with probability P(vk)P(v_k)P(v​k​​), is defined as:Entropy:H(V)=∑kP(vk)log21P(vk)=−∑kP(vk)log2P(vk)Entropy:H(V) = \\sum\\limits_k {P({v_k}){{\\log }_2}} {1 \\over {P({v_k})}} = - \\sum\\limits_k {P({v_k}){{\\log }_2}P({v_k})}Entropy:H(V)=​k​∑​​P(v​k​​)log​2​​​P(v​k​​)​​1​​=−​k​∑​​P(v​k​​)log​2​​P(v​k​​) An attribute XXX with ddd distinct values divides the training set EEE into E1E_1E​1​​, …, EdE_dE​d​​, and the entropy remaining after testing attribute XXX is:Reminder(X)=∑k=1d∣Ek∣∣E∣H(Ek){\\mathrm{Reminder}}(X) = \\sum\\limits_{k = 1}^d {{{|{E_k}|} \\over {|E|}}H({E_k})}Reminder(X)=​k=1​∑​d​​​∣E∣​​∣E​k​​∣​​H(E​k​​) where H(Ek)H({E_k})H(E​k​​) is the entropy that considered the set EkE_kE​k​​ as a random variable comes up an output value with a probability in direct proportional to the number of the examples in EkE_kE​k​​ with this output value. The information gain from the attribute test on XXX is the expected reduction in entropy: Gain(X)=H(E)−Reminder(X){\\mathrm{Gain(X) = H(E) - Reminder}}(X)Gain(X)=H(E)−Reminder(X) Based on the above formulas, we can figure the most “important” attribute that has the highest information gain.","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://tedxz.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://tedxz.github.io/tags/Machine-Learning/"},{"name":"Decision Tree","slug":"Decision-Tree","permalink":"http://tedxz.github.io/tags/Decision-Tree/"}]},{"title":"Machine Learning Note (1) - Reasons and forms of learning","slug":"ml-note-1-reasons-and-forms-of-learning","date":"2015-01-01T03:44:18.000Z","updated":"2016-07-26T16:37:23.599Z","comments":true,"path":"2015/01/01/ml-note-1-reasons-and-forms-of-learning/","link":"","permalink":"http://tedxz.github.io/2015/01/01/ml-note-1-reasons-and-forms-of-learning/","excerpt":"Why would we want an agent to learn? There are three main reasons. The designers cannot anticipate all possible situations the agent might find itself in. The designers cannot anticipate all changes over time. Sometimes human programmers have no idea how to program a solution themselves.","text":"Why would we want an agent to learn? There are three main reasons. The designers cannot anticipate all possible situations the agent might find itself in. The designers cannot anticipate all changes over time. Sometimes human programmers have no idea how to program a solution themselves. Forms of learningLearning techniques depend on for major factors: Component to be improved. Prior knowledge already has. (This and the following are fields under the charge of Liuqing Yang) Representation used for the data and the component. Feedback information. Different types of feedback lead to different types of learning. Unsupervised learning - No explicit feedback. Clustering. Reinforcement Learning - Agent gets rewards and punishments. Advertisement systems. Music &amp; shopping recommender system. (watching goods in Taobao is also learning machine learning :P) Supervised learning - Agent observes some example input with input and correct output. Credit approval. Semi-supervised learning - Few labeled examples and many unlabeled examples. Image recognition.","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://tedxz.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://tedxz.github.io/tags/Machine-Learning/"}]},{"title":"大三上学期的一点回顾","slug":"review-of-first-semester-in-junior","date":"2014-12-26T13:51:33.000Z","updated":"2016-07-26T16:41:30.765Z","comments":true,"path":"2014/12/26/review-of-first-semester-in-junior/","link":"","permalink":"http://tedxz.github.io/2014/12/26/review-of-first-semester-in-junior/","excerpt":"题目是《大三上学期的一点回顾》，其实大三上学期还没结束。我比较懒本打算拖到考完试再写这点总结，但是最近在实验室的学习任务开始了，也要每周有一些记录和总结，想到这BLOG也是个不错的记录总结的地方，不如就记录在此。","text":"题目是《大三上学期的一点回顾》，其实大三上学期还没结束。我比较懒本打算拖到考完试再写这点总结，但是最近在实验室的学习任务开始了，也要每周有一些记录和总结，想到这BLOG也是个不错的记录总结的地方，不如就记录在此。总的来说，这学期的收获还蛮大的，最首要的一个就是获得了一枚ACM-ICPC的银牌（差一点金），也算是对进入交大两年多一直以来对ACM热情的一点点回馈。附上Board。 下图左到右夏老板（Stumble），华哥（Ply_py），我（River Crab）–&gt; StRcPy，好吧顺序不太对。 另一个重要的点就是发现了机器学习这个有趣的领域。暑假的时候队友Ply_py叫我一起在Coursea让上follow一门网络公开课：台湾大学林轩田老师的《Machine Learning Foundation》课程，没想到竟然感觉很有意思，于是便坚持了下来，目前这个课程已经结束，对应台大机器学习下半学期课程的《Machine Learning Techniques》已经开始。上个月又join了同是台大的另一门课《人工智慧》，学到了人工智能的各种搜索技术（早已经会的各种搜索算法也学到了新的感受）和知识表示等内容。虽然网络公开课不像学校的课程那样强制，但是却感觉有趣得多，而且作业设计也很用心，让人很有做作业的欲望（？）哈哈。假期打算听一下Andrew Ng的《Machine Learning》，巩固一下，也当学英语啦。 前些日子报名进入了学校的计算机网络技术与工程研究所，我选择的方向主要负责一些机器学习、数据挖掘算法的实现。我打算把CSDN作为一个记录学习过程和问题的地方，也把前些日子学到的机器学习和人工智能的知识在这里简单的总结一点点。以往总是发现以前学过的东西过一段时间之后又生疏了，想记录又苦于没有安排好合适的时间养成习惯，这次以加入实验室为契机，希望能养成这个习惯。","categories":[{"name":"Diary","slug":"Diary","permalink":"http://tedxz.github.io/categories/Diary/"}],"tags":[{"name":"Diary","slug":"Diary","permalink":"http://tedxz.github.io/tags/Diary/"},{"name":"Personal Review","slug":"Personal-Review","permalink":"http://tedxz.github.io/tags/Personal-Review/"}]},{"title":"Linux/Debian下安装OpenGL的方法","slug":"installing-opengl-in-linux","date":"2014-10-28T06:00:10.000Z","updated":"2015-02-03T08:59:45.296Z","comments":true,"path":"2014/10/28/installing-opengl-in-linux/","link":"","permalink":"http://tedxz.github.io/2014/10/28/installing-opengl-in-linux/","excerpt":"","text":"安装mesa和freeglut：12sudo apt-get install freeglut3 freeglut3-devsudo apt-get install mesa-common-dev mesa-utils 程序需要包含库：123#include \"GL/gl.h\"#include \"GL/glu.h\"#include \"GL/glut.h\" 编译命令实例：1g++ -lgl -lglu -lglut -o test test.cpp","categories":[{"name":"Skill","slug":"Skill","permalink":"http://tedxz.github.io/categories/Skill/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://tedxz.github.io/tags/Linux/"}]},{"title":"Linux和Windows双系统时间同步问题","slug":"linux-windows-time-sync","date":"2014-10-28T05:38:17.000Z","updated":"2015-02-03T08:24:50.184Z","comments":true,"path":"2014/10/28/linux-windows-time-sync/","link":"","permalink":"http://tedxz.github.io/2014/10/28/linux-windows-time-sync/","excerpt":"","text":"修改 Windows 使用格林威治标准时间 UTC方法如下：HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\TimeZoneInformation\\ 中加一项类型为 REG_DWORD 的机码，名称为 RealTimeIsUniversal，值设为 1。然后更新一下系统时间即可。 修改 Ubuntu 不使用格林威治标准时间 UTC (启用本地时间) 方法如下：修改/etc/default/rcS原来的设定UTC=yes为UTC=no。 修改Linux的时间存储方式就是去改/etc/sysconfig/clock 这个文件（Redhat和Fedora里面有的）这个文件的内容如下123456# cat /etc/sysconfig/clock# The ZONE parameter is only evaluated by system-config-date.# The timezone of the system is defined by the contents of /etc/localtime.ZONE=&quot;Asia/Shanghai&quot;UTC=falseARC=false # cat /etc/sysconfig/clock","categories":[{"name":"Skill","slug":"Skill","permalink":"http://tedxz.github.io/categories/Skill/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://tedxz.github.io/tags/Linux/"},{"name":"Skill","slug":"Skill","permalink":"http://tedxz.github.io/tags/Skill/"},{"name":"Dualboot","slug":"Dualboot","permalink":"http://tedxz.github.io/tags/Dualboot/"}]}]}